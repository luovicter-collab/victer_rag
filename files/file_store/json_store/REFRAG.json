{
  "metadata": {
    "doc_id": "REFRAG",
    "doc_title": "REFRAG",
    "parse_stage": "region_divided",
    "language": "en",
    "source_file": "REFRAG.pdf",
    "pdf_path": "D:/Code_list/Code_Python/Agent_gragh_rag/files/file_store/pdf_store/REFRAG.pdf",
    "total_pages": 30,
    "total_elements": 166,
    "region_division": {
      "head": {
        "start_seq": 1,
        "end_seq": 3
      },
      "body": {
        "start_seq": 4,
        "end_seq": 72
      },
      "tail": {
        "start_seq": 73,
        "end_seq": 166
      }
    }
  },
  "elements": [
    {
      "id": "REFRAG_elem_000001",
      "type": "title",
      "content": {
        "text": "REFRAG: Rethinking RAG based Decoding ",
        "level": 1
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 0,
        "bbox": {
          "x1": 138,
          "y1": 99,
          "x2": 741,
          "y2": 125
        }
      },
      "metadata": {
        "char_count": 38
      }
    },
    {
      "id": "REFRAG_elem_000002",
      "type": "paragraph",
      "content": {
        "text": "Xiaoqiang Lin $^ { 1 , 2 , * }$, Aritra Ghosh1, Bryan Kian Hsiang Low2, Anshumali Shrivastava1,3, Vijai Mohan1 1Meta Superintelligence Labs, 2National University of Singapore, 3Rice University $^ *$Work done at Meta Large Language Models (LLMs) have demonstrated remarkable capabilities in leveraging extensive external knowledge to enhance responses in multi-turn and agentic applications, such as retrievalaugmented generation (RAG). However, processing long-context inputs introduces significant system latency and demands substantial memory for the key-value cache, resulting in reduced throughput and a fundamental trade-off between knowledge enrichment and system efficiency. While minimizing latency for long-context inputs is a primary objective for LLMs, we contend that RAG systems require specialized consideration. In RAG, much of the LLM context consists of concatenated passages from retrieval, with only a small subset directly relevant to the query. These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to block-diagonal attention patterns that differ from those in standard LLM generation tasks. Based on this observation, we argue that most computations over the RAG context during decoding are unnecessary and can be eliminated with minimal impact on performance. To this end, we propose REFRAG, an efficient decoding framework that compresses, senses, and expands to improve latency in RAG applications. By exploiting this attention sparsity structure, we demonstrate a 30.85 $\\times$the time-to-first-token acceleration (3.75 $\\times$improvement to previous work) without loss in perplexity. In addition, our optimization framework for large context enables REFRAG to extend the context size of LLMs by $1 6 \\times$. We provide rigorous validation of REFRAG across diverse long-context tasks, including RAG, multi-turn conversations, and long document summarization, spanning a wide range of datasets. Experimental results confirm that REFRAG delivers substantial speedup with no loss in accuracy compared to LLaMA models and other state-of-the-art baselines across various context sizes. Additionally, our experiments establish that the expanded context window of REFRAG further enhances accuracy for popular applications. "
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 0,
        "bbox": {
          "x1": 137,
          "y1": 130,
          "x2": 816,
          "y2": 147
        },
        "section_title": "REFRAG: Rethinking RAG based Decoding "
      },
      "metadata": {
        "char_count": 2306
      }
    },
    {
      "id": "REFRAG_elem_000003",
      "type": "paragraph",
      "content": {
        "text": "Date: October 14, 2025 Correspondence: Aritra Ghosh at arighosh@meta.com Code: Will be available at https://github.com/facebookresearch/refrag $\\infty$Meta "
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 0,
        "bbox": {
          "x1": 138,
          "y1": 551,
          "x2": 295,
          "y2": 566
        },
        "section_title": "REFRAG: Rethinking RAG based Decoding "
      },
      "metadata": {
        "char_count": 156
      }
    },
    {
      "id": "REFRAG_elem_000004",
      "type": "title",
      "content": {
        "text": "1 Introduction ",
        "level": 1
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 0,
        "bbox": {
          "x1": 109,
          "y1": 642,
          "x2": 269,
          "y2": 659
        }
      },
      "metadata": {
        "char_count": 15
      }
    },
    {
      "id": "REFRAG_elem_000005",
      "type": "paragraph",
      "content": {
        "text": "Large Language Models (LLMs) have demonstrated impressive capabilities in contextual learning, leveraging information from their input to achieve superior performance across a range of downstream applications. For instance, in multi-turn conversations (Roller et al., 2021; Zhang et al., 2020), incorporating historical dialogue into the context enables LLMs to respond more effectively to user queries. In retrieval-augmented generation (RAG) (Guu et al., 2020; Izacard et al., 2022), LLMs generate more accurate answers by utilizing relevant search results retrieved from external sources. These examples highlight the power of LLMs to learn from context. However, it is well established that increasing prompt length for contextual learning leads to higher latency and greater memory consumption during inference (Yen et al., 2024). Specifically, longer prompts require additional memory for the key-value (KV) cache, which scales linearly with prompt length. Moreover, the time-to-first-token (TTFT) latency increases quadratically, while the time-to-iterative-token (TTIT) latency grows linearly with prompt length (Liu et al., 2025). As a result, LLM inference throughput degrades with larger contexts, limiting their applicability in scenarios demanding high throughput and low latency, such as web-scale discovery. Therefore, developing novel model architectures that optimize memory usage and inference latency is crucial for enhancing the practicality of contextual learning in these applications. "
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 0,
        "bbox": {
          "x1": 107,
          "y1": 674,
          "x2": 887,
          "y2": 886
        },
        "section_title": "1 Introduction "
      },
      "metadata": {
        "char_count": 1508
      }
    },
    {
      "id": "REFRAG_elem_000006",
      "type": "paragraph",
      "content": {
        "text": "Optimizing inference latency for LLMs with extensive context is an active area of research, with approaches ranging from modifying the attention mechanism’s complexity (Beltagy et al., 2020) to sparsifying attention and context (Child et al., 2019; Xiao et al., 2024; Jiang et al., 2024), and altering context feeding strategies (Yen et al., 2024). However, most existing methods target generic LLM tasks with long context and are largely orthogonal to our work. This paper focuses on RAG-based applications, such as web-scale search, with the goal of improving inference latency, specifically, the TTFT. We argue that specialized techniques exploiting the unique structure and sparsity inherent in RAG contexts can substantially reduce memory and computational overhead. Treating RAG TTFT as a generic LLM inference problem overlooks several key aspects: 1) Inefficient Token Allocation. RAG contexts often contain sparse information, with many retrieved passages being uninformative and reused across multiple inferences. Allocating memory/computation for all the tokens, as we show in this paper, is unnecessarily wasteful. 2) Wasteful Encoding and Other Information. The retrieval process in RAG has already pre-processed the chunks of the contexts, and their encodings and other correlations with the query are already available due to the use of vectorizations and re-rankings. This information is discarded during decoding. 3) Unusually Structured and Sparse Attention. Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated, resulting in predominantly zero cross-attention between chunks (see figure 7). "
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 0,
        "bbox": {
          "x1": 109,
          "y1": 893,
          "x2": 883,
          "y2": 909
        },
        "section_title": "1 Introduction "
      },
      "metadata": {
        "char_count": 1668
      }
    },
    {
      "id": "REFRAG_elem_000007",
      "type": "title",
      "content": {
        "text": "1.1 Our Contributions ",
        "level": 1
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 1,
        "bbox": {
          "x1": 109,
          "y1": 325,
          "x2": 305,
          "y2": 342
        }
      },
      "metadata": {
        "char_count": 22
      }
    },
    {
      "id": "REFRAG_elem_000008",
      "type": "paragraph",
      "content": {
        "text": "We propose REFRAG (REpresentation For RAG), a novel mechanism for efficient decoding of contexts in RAG. REFRAG significantly reduces latency, TTFT, and memory usage during decoding, all without requiring modifications to the LLM architecture or introducing new decoder parameters. "
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 1,
        "bbox": {
          "x1": 109,
          "y1": 349,
          "x2": 887,
          "y2": 397
        },
        "section_title": "1.1 Our Contributions "
      },
      "metadata": {
        "char_count": 282
      }
    },
    {
      "id": "REFRAG_elem_000009",
      "type": "paragraph",
      "content": {
        "text": "REFRAG makes several novel modifications to the decoding process: Instead of using tokens from retrieved passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context. Unlike prior methods (Yen et al., 2024), REFRAG supports compression of token chunks at arbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting multi-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight reinforcement learning (RL) policy that selectively determines when full chunk token input is necessary and when low-cost, approximate chunk embeddings suffice . As a result, REFRAG minimizes reliance on computationally intensive token embeddings, condensing most chunks for the query in RAG settings. "
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 1,
        "bbox": {
          "x1": 109,
          "y1": 402,
          "x2": 888,
          "y2": 587
        },
        "section_title": "1.1 Our Contributions "
      },
      "metadata": {
        "char_count": 1268
      }
    },
    {
      "id": "REFRAG_elem_000010",
      "type": "paragraph",
      "content": {
        "text": "We provide rigorous experimental validations of the effectiveness of REFRAG in continual pre-training and many real word long-context applications including RAG, multi-turn conversation with RAG and long document summarization. Results show that we achieve 30.75 $\\times$TTFT acceleration without loss in perplexity which is $3 . 7 5 \\times$than previous method. Moreover, with extended context due to our compression, REFRAG achieves better performance than LLaMA without incurring higher latency in the downstream applications. "
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 1,
        "bbox": {
          "x1": 109,
          "y1": 592,
          "x2": 887,
          "y2": 671
        },
        "section_title": "1.1 Our Contributions "
      },
      "metadata": {
        "char_count": 530
      }
    },
    {
      "id": "REFRAG_elem_000011",
      "type": "title",
      "content": {
        "text": "2 Model Architecture ",
        "level": 1
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 1,
        "bbox": {
          "x1": 109,
          "y1": 691,
          "x2": 344,
          "y2": 709
        }
      },
      "metadata": {
        "char_count": 21
      }
    },
    {
      "id": "REFRAG_elem_000012",
      "type": "paragraph",
      "content": {
        "text": "We denote the decoder model as ${ \\mathcal { M } } _ { \\mathrm { d e c } }$and the encoder model as $\\mathcal { M } _ { \\mathrm { e n c } }$. Given an input with $T$tokens $x _ { 1 } , x _ { 2 } , \\ldots , x _ { T }$, we assume that the first $q$tokens are main input tokens (e.g., questions) and the last $s$tokens are context tokens (e.g., retrieved passages in RAG). We have $q + s = T$. For clarity, we focus on a single turn of question and retrieval in this section. "
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 1,
        "bbox": {
          "x1": 109,
          "y1": 723,
          "x2": 887,
          "y2": 785
        },
        "section_title": "2 Model Architecture "
      },
      "metadata": {
        "char_count": 473
      }
    },
    {
      "id": "REFRAG_elem_000013",
      "type": "paragraph",
      "content": {
        "text": "Model overview. Figure 1 shows the main architecture of REFRAG. This model consists of a decoder-only foundation model (e.g., LLaMA (Touvron et al., 2023)) and a lightweight encoder model (e.g., Roberta (Liu et al., 2019)). When given a question $x _ { 1 } , \\ldots , x _ { q }$and context $x _ { q + 1 } , \\dots , x _ { T }$and , the context is chunked into $\\begin{array} { r } { L : = \\frac { s } { k } } \\end{array}$number of $k$-sized chunks $\\{ C _ { 1 } , \\dots , C _ { L } \\}$where $C _ { i } = \\{ x _ { q + k * i } , \\dots , x _ { q + k * i + k - 1 } \\}$. The encoder model then processes all the chunks to obtain a chunk embedding for each chunk $\\mathbf { c } _ { i } = { \\mathcal { M } } _ { \\mathrm { e n c } } ( C _ { i } )$. This chunk embedding is then projected with a projection layer $\\phi$to match the size of the token embedding of the decoder model, $\\mathbf { e } _ { i } ^ { \\mathrm { { c n k } } } = \\phi ( \\mathbf { { c } } _ { i } )$. These projected chunk embeddings are then fed to the decoder model along with the token embeddings for the question to generate the answer $\\boldsymbol { y } \\sim \\mathcal { M } _ { \\mathrm { d e c } } ( \\{ \\mathbf { e } _ { 1 } , \\dots , \\mathbf { e } _ { q } , \\mathbf { e } _ { 1 } ^ { \\mathrm { c n k } } , \\dots , \\mathbf { e } _ { L } ^ { \\mathrm { c n k } } \\} )$) where $\\mathbf { e } _ { i }$is the "
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 1,
        "bbox": {
          "x1": 109,
          "y1": 791,
          "x2": 888,
          "y2": 916
        },
        "section_title": "2 Model Architecture "
      },
      "metadata": {
        "char_count": 1370
      }
    },
    {
      "id": "REFRAG_elem_000014",
      "type": "image",
      "content": {
        "captions": [
          "Figure 1 The main design of REFRAG. The input context is chunked and processed by the light-weight encoder to produce chunk embeddings, which are precomputable for efficient reuse. A light-weight RL policy decide few chunks to expand. These chunk embeddings along with the token embeddings of the question input are fed to the decoder. "
        ],
        "description": ""
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 2,
        "bbox": {
          "x1": 133,
          "y1": 95,
          "x2": 867,
          "y2": 314
        },
        "section_title": "2 Model Architecture ",
        "image_path": "D:/Code_list/Code_Python/Agent_gragh_rag/files/file_store/md_store/minerU_work/REFRAG/images/6bf188d6c1aba2cf8ab434508705ccdd967b705f54e1729a6be557084bc17910.jpg"
      }
    },
    {
      "id": "REFRAG_elem_000015",
      "type": "paragraph",
      "content": {
        "text": "token embedding for token $x _ { i }$. In real applications (e.g., RAG), the context is the dominating part of the input (i.e., $s \\gg q$) and hence the overall input to the decoder will be reduced by a factor of $\\simeq k$. This architectural design leads to significant reductions in both latency and memory usage, primarily due to the shortened input sequence. Additionally, an RL policy is trained to do selective compression to further improve the performance which we will defer the discussion to section 2. Next, we analyze the system performance gains achieved with a compression rate of $k$. "
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 2,
        "bbox": {
          "x1": 109,
          "y1": 406,
          "x2": 883,
          "y2": 497
        },
        "section_title": "2 Model Architecture "
      },
      "metadata": {
        "char_count": 601
      }
    },
    {
      "id": "REFRAG_elem_000016",
      "type": "image",
      "content": {
        "captions": [
          "Figure 2 Empirical verification of inference acceleration of REFRAG with ",
          "k = 1 6",
          ". "
        ],
        "description": ""
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 2,
        "bbox": {
          "x1": 617,
          "y1": 512,
          "x2": 841,
          "y2": 652
        },
        "section_title": "2 Model Architecture ",
        "image_path": "D:/Code_list/Code_Python/Agent_gragh_rag/files/file_store/md_store/minerU_work/REFRAG/images/fc83abe9d14e1b80ee0f493733d78461e42c3a7f8ca91f03b9ad6a8bb5d4bbdc.jpg"
      }
    },
    {
      "id": "REFRAG_elem_000017",
      "type": "paragraph",
      "content": {
        "text": "Latency and throughput improvement. We evaluate three metrics: TTFT, the latency to generate the first token; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated per unit time. Theoretical analysis (section A) shows that for short context lengths, our method achieves up to $k \\times$acceleration in TTFT and throughput. For longer context length, acceleration reaches up to $k ^ { 2 } \\times$for both metrics. Empirically, as shown in figure 2, with a context length of 16384 (mid-to-long context), REFRAG with $k = 1 6$achieves $1 6 . 5 3 \\times$TTFT acceleration with cache and 8.59 $\\times$without cache1, both surpassing CEPE $( 2 . 0 1 \\times$and $1 . 0 4 \\times$, respectively), while achieving $9 . 3 \\%$performance (measured by log-perplexity) compared to CEPE (table 1). We achieve up to $6 . 7 8 \\times$throughput acceleration compared to LLaMA, significantly outperforming CEPE. With $k = 3 2$, TTFT acceleration reaches 32.99 $\\times$compared to LLaMA (3.75× compared to CEPE) while maintaining similar performance to CEPE (see figure 8 and table 2). More detailed discussion on empirical evaluation is in section A. "
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 2,
        "bbox": {
          "x1": 109,
          "y1": 733,
          "x2": 883,
          "y2": 885
        },
        "section_title": "2 Model Architecture "
      },
      "metadata": {
        "char_count": 1175
      }
    },
    {
      "id": "REFRAG_elem_000018",
      "type": "title",
      "content": {
        "text": "3 Methodology ",
        "level": 1
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 3,
        "bbox": {
          "x1": 109,
          "y1": 119,
          "x2": 282,
          "y2": 138
        }
      },
      "metadata": {
        "char_count": 14
      }
    },
    {
      "id": "REFRAG_elem_000019",
      "type": "paragraph",
      "content": {
        "text": "To align the encoder and decoder, we follow the work of Yen et al. (2024) to use the next paragraph prediction tasks for continual pre-training (CPT). Specifically, for each data data point, it contains $s + o = T$number of tokens, which we use for CPT to prepare the model for downstream tasks utilizing chunk embeddings. To further enhance performance, we introduce selective compression via RL. After aligning the encoder and decoder through CPT, we apply supervised fine-tuning (SFT) to adapt the model to specific downstream tasks, such as RAG and multi-turn conversation. Additional details are provided in section 5. "
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 3,
        "bbox": {
          "x1": 109,
          "y1": 152,
          "x2": 887,
          "y2": 243
        },
        "section_title": "3 Methodology "
      },
      "metadata": {
        "char_count": 624
      }
    },
    {
      "id": "REFRAG_elem_000020",
      "type": "paragraph",
      "content": {
        "text": "During CPT, we input the first $s$tokens $x _ { 1 : s }$into the encoder and use its output to assist the decoder in predicting the next $o$tokens $x _ { s + 1 : s + o }$. This task encourages the model to leverage contextual information for next-paragraph prediction, thereby equipping it for downstream applications. The objective is to align any encoder–decoder combination so that the generations produced with compressed context closely resemble those generated by the original decoder with access to the full context. "
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 3,
        "bbox": {
          "x1": 109,
          "y1": 250,
          "x2": 885,
          "y2": 325
        },
        "section_title": "3 Methodology "
      },
      "metadata": {
        "char_count": 524
      }
    },
    {
      "id": "REFRAG_elem_000021",
      "type": "title",
      "content": {
        "text": "3.1 Continual Pre-training Recipe ",
        "level": 1
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 3,
        "bbox": {
          "x1": 109,
          "y1": 345,
          "x2": 398,
          "y2": 362
        }
      },
      "metadata": {
        "char_count": 34
      }
    },
    {
      "id": "REFRAG_elem_000022",
      "type": "paragraph",
      "content": {
        "text": "To ensure the success of the CPT phase, we propose a training recipe that incorporates a reconstruction task and a curriculum learning approach. Ablation studies in section 4 demonstrate that this recipe is crucial for achieving strong CPT performance. "
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 3,
        "bbox": {
          "x1": 109,
          "y1": 369,
          "x2": 885,
          "y2": 415
        },
        "section_title": "3.1 Continual Pre-training Recipe "
      },
      "metadata": {
        "char_count": 253
      }
    },
    {
      "id": "REFRAG_elem_000023",
      "type": "paragraph",
      "content": {
        "text": "Reconstruction task. We input the first s tokens $x _ { 1 : s }$to the encoder and learn to reconstruct tokens $x _ { 1 : s }$in the decoder. In this task, we freeze the decoder model and only train the encoder and projection layer. The main objectives are to align the encoder and projection layer so that: 1) encoder can compress $k$tokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information. The reconstruction task was specifically chosen to encourage the model to rely on context memory rather than its parametric memory during training. Once the encoder is aligned with the decoder through this reconstruction task, we initiate CPT by unfreezing the decoder. "
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 3,
        "bbox": {
          "x1": 109,
          "y1": 422,
          "x2": 887,
          "y2": 542
        },
        "section_title": "3.1 Continual Pre-training Recipe "
      },
      "metadata": {
        "char_count": 837
      }
    },
    {
      "id": "REFRAG_elem_000024",
      "type": "paragraph",
      "content": {
        "text": "Curriculum learning. The training tasks described in the previous section may seem straightforward, but they are inherently complex. As the chunk length $k$increases, the number of possible token combinations expands exponentially, specifically at a rate of $V ^ { k }$, where $V$is the vocabulary size. Effectively capturing this diversity within a fixed-length embedding presents a significant challenge. Additionally, reconstructing $s = k \\times L$tokens from $L$chunk embeddings further compounds the difficulty of the task. "
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 3,
        "bbox": {
          "x1": 109,
          "y1": 550,
          "x2": 887,
          "y2": 626
        },
        "section_title": "3.1 Continual Pre-training Recipe "
      },
      "metadata": {
        "char_count": 530
      }
    },
    {
      "id": "REFRAG_elem_000025",
      "type": "paragraph",
      "content": {
        "text": "Counterintuitively, directly continuing pre-training of the decoder to utilize encoder outputs did not reduce perplexity, even for the reconstruction task. To address the optimization challenge, we propose employing curriculum learning for both tasks. Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills. For the reconstruction task, training begins with reconstructing a single chunk: the encoder receives one chunk embedding $\\mathbf { c } _ { 1 }$for $x _ { 1 : k }$and and the decoder reconstructs the $k$tokens using the projected chunk embedding ${ \\bf e } _ { 1 } ^ { \\mathrm { c n k } }$. Subsequently, the model reconstructs $x _ { 1 : 2 k }$from ${ \\bf e } _ { 1 } ^ { \\mathrm { c n k } } , { \\bf e } _ { 2 } ^ { \\mathrm { c n k } }$, and so forth. To continuously adjust task difficulty, we vary the data mixture over time, starting with examples dominated by easier tasks (e.g., single chunk embedding) and gradually shifting towards those dominated by more difficult tasks (i.e., $L$chunk embeddings). A visualization of the data mixture during curriculum learning is provided in figure 6, and the detailed scheduling is presented in table 8. "
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 3,
        "bbox": {
          "x1": 109,
          "y1": 633,
          "x2": 887,
          "y2": 785
        },
        "section_title": "3.1 Continual Pre-training Recipe "
      },
      "metadata": {
        "char_count": 1236
      }
    },
    {
      "id": "REFRAG_elem_000026",
      "type": "paragraph",
      "content": {
        "text": "Selective compression REFRAG introduces selective token compression, expanding important context chunks uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form. The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks. The policy network leverages chunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s autoregressive property and enabling flexible placement of compression. Further discussion on sequential selection is provided in section A.1. "
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 3,
        "bbox": {
          "x1": 109,
          "y1": 791,
          "x2": 887,
          "y2": 898
        },
        "section_title": "3.1 Continual Pre-training Recipe "
      },
      "metadata": {
        "char_count": 662
      }
    },
    {
      "id": "REFRAG_elem_000027",
      "type": "title",
      "content": {
        "text": "4 Experimental Results ",
        "level": 1
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 4,
        "bbox": {
          "x1": 109,
          "y1": 80,
          "x2": 362,
          "y2": 99
        }
      },
      "metadata": {
        "char_count": 23
      }
    },
    {
      "id": "REFRAG_elem_000028",
      "type": "paragraph",
      "content": {
        "text": "Training datasets. We use the Slimpajama dataset (Soboleva et al., 2023), an open source dataset for LLM pretraining. This dataset contains data from Wikipedia, Arxiv, Books, StackExchange, GitHub, Commoncrawl, C4. We only use the Book and ArXiv domains from the dataset since these two domains contain long texts (Yen et al., 2024). We sampled from this dataset to construct a 20B token training dataset which contains $5 0 \\%$data from Arxiv and 50% data from Book. "
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 4,
        "bbox": {
          "x1": 109,
          "y1": 113,
          "x2": 887,
          "y2": 189
        },
        "section_title": "4 Experimental Results "
      },
      "metadata": {
        "char_count": 468
      }
    },
    {
      "id": "REFRAG_elem_000029",
      "type": "paragraph",
      "content": {
        "text": "Evaluation datasets. We report the performance on the Book and ArXiv domain from Slimpajama which are hold out for evaluation only. To inspect the generalization of the model, we also report results on the PG19 (Rae et al., 2019) and Proof-pile datasets (Azerbayev et al., 2023). "
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 4,
        "bbox": {
          "x1": 109,
          "y1": 196,
          "x2": 885,
          "y2": 242
        },
        "section_title": "4 Experimental Results "
      },
      "metadata": {
        "char_count": 280
      }
    },
    {
      "id": "REFRAG_elem_000030",
      "type": "paragraph",
      "content": {
        "text": "Baselines. All baseline models are based on LLaMA-2-7B (Touvron et al., 2023), unless otherwise specified, to ensure fair comparison with prior work (Yen et al., 2024; Shi et al., 2024). Each data point contains $T = 4 0 9 6$tokens, split into $s = 2 0 4 8$context and $o = 2 0 4 8$output tokens. We evaluate perplexity on $x _ { s + 1 : s + o }$. Below, we briefly describe the main baselines; further details are provided in section B. LLaMA-No Context: LLaMA-2-7B evaluated on $x _ { s + 1 : s + o }$with only output tokens as input. LLaMA-Full Context: LLaMA-2-7B evaluated on $x _ { s + 1 : s + o }$with the full sequence $x _ { 1 : T }$as input. CEPE: Memory-efficient long-context model (Yen et al., 2024) a previous SOTA model which share some similarity to REFRAG CEPED denotes its instruction-tuned variant. LLaMA-32K: LLaMA-2-7B fine-tuned for 32K context length. REPLUG: Retrieval-augmented LLaMA-2-7B (Shi et al., 2024). REFRAG: Our approach (see Figure 1); REFRAG $k$denotes compression rate $k$, REFRAGRL uses RL-based selective compression. LLaMAK: LLaMA-2-7B evaluated on $x _ { s + 1 : s + o }$with the truncated sequence $x _ { s - K : T }$as input to match the token count of REFRAG. "
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 4,
        "bbox": {
          "x1": 109,
          "y1": 250,
          "x2": 887,
          "y2": 416
        },
        "section_title": "4 Experimental Results "
      },
      "metadata": {
        "char_count": 1204
      }
    },
    {
      "id": "REFRAG_elem_000031",
      "type": "paragraph",
      "content": {
        "text": "Table 1 reports performance for $s = 2 0 4 8$and $o \\in \\{ 5 1 2 , 1 0 2 4 , 2 0 4 8 \\}$, where, e.g., P512 denotes $o = 5 1 2$. Bolded results compare baselines, excluding LLaMA-Full Context and LLaMA-32K, which use full context without compression and are expected to perform best. Notably, REFRAG8 and REFRAG $^ { 1 6 }$consistently outperform other baselines across nearly all settings, while also achieving lower latency than CEPE (figure 2). For reference, LLaMA256 uses only the last 256 tokens, matching the number of chunk embeddings in REFRAG8 ( $s / k = 2 5 6 )$), yet REFRAG8 consistently surpasses LLaMA $^ { 2 5 6 }$, demonstrating the effectiveness of compressed chunk embeddings. "
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 4,
        "bbox": {
          "x1": 109,
          "y1": 422,
          "x2": 887,
          "y2": 529
        },
        "section_title": "4 Experimental Results "
      },
      "metadata": {
        "char_count": 696
      }
    },
    {
      "id": "REFRAG_elem_000032",
      "type": "paragraph",
      "content": {
        "text": "Table 2 evaluates $o = 2 0 4 8$with extended context lengths $s \\in \\{ 4 0 9 6 , 8 1 9 2 , 1 6 3 8 4 \\}$. Although our model is trained on $s + o = 6 1 4 4$, both REFRAG8 and REFRAG $^ { 1 6 }$maintain superior performance at longer contexts. The original Llama-2-7B supports only a 4k context window, whereas our approach enables extrapolation via chunk embeddings, extending context and supporting broader applications. "
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 4,
        "bbox": {
          "x1": 109,
          "y1": 536,
          "x2": 887,
          "y2": 597
        },
        "section_title": "4 Experimental Results "
      },
      "metadata": {
        "char_count": 422
      }
    },
    {
      "id": "REFRAG_elem_000033",
      "type": "paragraph",
      "content": {
        "text": "With a compression rate of 16, we achieve a 9.3% average log-perplexity improvement over CEPE across four datasets2. Meanwhile, our method is 16.53 $\\times$faster than LLaMA in TTFT and 2.01 $\\times$faster than CEPE (section B.4). At a compression rate of 32, our log-perplexity matches CEPE, while TTFT acceleration increases to 30.85 $\\times$over LLaMA and 3.75 $\\times$over CEPE. "
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 4,
        "bbox": {
          "x1": 109,
          "y1": 604,
          "x2": 887,
          "y2": 665
        },
        "section_title": "4 Experimental Results "
      },
      "metadata": {
        "char_count": 383
      }
    },
    {
      "id": "REFRAG_elem_000034",
      "type": "paragraph",
      "content": {
        "text": "chunks in the original token space using the RL policy. The effective compression rate Figure 3 presents the performance of various methods for selective compression. We expand $\\frac { k } { 1 - p + k p }$$p$fraction of the decreases when fewer chunks are compressed (i.e., $p$increases). We compare the perplexity of $x _ { s + 1 : s + o }$using different selection policy under different $p$. The perplexity-based selection is an heuristic based selection which compresses chunks with low perplexity (Perplexity-desc) or high perplexity (Perplexity-asc). The perplexity is measured by the LLaMA-2-7B model. Intuitively, a chunk with lower perplexity contains less information and can therefore be compressed with minimal information loss. Ideally, this approach should outperform random selection, which is indeed observed in figure 3. The RL-based selective compression policy consistently achieves superior performance across varying compression rates $p$. "
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 4,
        "bbox": {
          "x1": 109,
          "y1": 672,
          "x2": 887,
          "y2": 810
        },
        "section_title": "4 Experimental Results "
      },
      "metadata": {
        "char_count": 962
      }
    },
    {
      "id": "REFRAG_elem_000035",
      "type": "title",
      "content": {
        "text": "4.1 Ablation Study ",
        "level": 1
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 4,
        "bbox": {
          "x1": 109,
          "y1": 829,
          "x2": 282,
          "y2": 845
        }
      },
      "metadata": {
        "char_count": 19
      }
    },
    {
      "id": "REFRAG_elem_000036",
      "type": "paragraph",
      "content": {
        "text": "Curriculum learning is essential for effective training in the reconstruction task. The reconstruction task, while intuitive, is particularly challenging when multiple chunks must be reconstructed. Table 11 shows the "
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 4,
        "bbox": {
          "x1": 109,
          "y1": 853,
          "x2": 885,
          "y2": 883
        },
        "section_title": "4.1 Ablation Study "
      },
      "metadata": {
        "char_count": 217
      }
    },
    {
      "id": "REFRAG_elem_000037",
      "type": "table",
      "content": {
        "html": "<table><tr><td></td><td colspan=\"3\">Arxiv</td><td colspan=\"3\">Book</td><td colspan=\"3\">PG19</td><td colspan=\"3\">ProofPile</td></tr><tr><td></td><td>P512</td><td>P1024</td><td>P2048</td><td>P512</td><td>P1024</td><td>P2048</td><td>P512</td><td>P1024</td><td>P2048</td><td>P512</td><td>P1024</td><td>P2048 ↓</td></tr><tr><td>LLAMA-FULL CONTEXT</td><td>1.075</td><td>1.074</td><td>1.069</td><td>1.830</td><td>1.827</td><td>1.826</td><td>1.947</td><td>1.941</td><td>1.935</td><td>0.952</td><td>0.940</td><td>0.931</td></tr><tr><td>LLAMA-32K</td><td>1.086</td><td>1.084</td><td>1.076</td><td>1.887</td><td>1.883</td><td>1.880</td><td>1.988</td><td>1.982</td><td>1.975</td><td>0.961</td><td>0.948</td><td>0.937</td></tr><tr><td>LLAMA-No CONTEXT</td><td>1.526</td><td>1.371</td><td>1.254</td><td>2.101</td><td>1.995</td><td>1.927</td><td>2.211</td><td>2.102</td><td>2.030</td><td>1.437</td><td>1.256</td><td>1.127</td></tr><tr><td>\\( LLAMA_{256} \\)</td><td>1.267</td><td>1.221</td><td>1.171</td><td>1.924</td><td>1.897</td><td>1.874</td><td>2.031</td><td>2.003</td><td>1.978</td><td>1.156</td><td>1.094</td><td>1.038</td></tr><tr><td>REPLUG</td><td>1.526</td><td>1.371</td><td>1.254</td><td>2.101</td><td>1.995</td><td>1.927</td><td>2.211</td><td>2.102</td><td>2.030</td><td>1.437</td><td>1.256</td><td>1.127</td></tr><tr><td>CEPE</td><td>1.196</td><td>1.148</td><td>1.107</td><td>1.946</td><td>1.896</td><td>1.864</td><td>2.057</td><td>2.002</td><td>1.964</td><td>1.075</td><td>1.014</td><td>0.968</td></tr><tr><td>\\( REFRAG_8 \\)</td><td>1.124</td><td>1.091</td><td>1.062</td><td>1.905</td><td>1.868</td><td>1.844</td><td>1.996</td><td>1.956</td><td>1.927</td><td>0.997</td><td>0.952</td><td>0.916</td></tr><tr><td>\\( REFRAG_{16} \\)</td><td>1.157</td><td>1.114</td><td>1.076</td><td>1.925</td><td>1.882</td><td>1.853</td><td>2.016</td><td>1.971</td><td>1.938</td><td>1.034</td><td>0.976</td><td>0.931</td></tr><tr><td>\\( REFRAG_{32} \\)</td><td>1.215</td><td>1.154</td><td>1.103</td><td>1.946</td><td>1.896</td><td>1.862</td><td>2.039</td><td>1.987</td><td>1.949</td><td>1.097</td><td>1.020</td><td>0.961</td></tr></table>",
        "captions": [
          "Table 1 Log-Perplexity on output tokens ",
          "x _ { s + 1 : s + o }",
          "given context tokens ",
          "x _ { 1 : s }",
          "for different models. We use ",
          "s = 2 0 4 8",
          "and ",
          "o \\in \\{ 5 1 2 , 1 0 2 4 , 2 0 4 8 \\}",
          "here. Bolding are based on comparing baselines excluding LLaMA-Full Context and LLaMA-32K since they are expected to be the best (ideally). The lower the better (↓). "
        ],
        "description": ""
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 5,
        "bbox": {
          "x1": 114,
          "y1": 132,
          "x2": 885,
          "y2": 267
        },
        "section_title": "4.1 Ablation Study ",
        "image_path": "D:/Code_list/Code_Python/Agent_gragh_rag/files/file_store/md_store/minerU_work/REFRAG/images/fb166c4a669d6f86db47203f418e41b4a2e3beff209f1bb8405d580fb286ea00.jpg"
      },
      "metadata": {
        "table_type": "complex_table",
        "row_count": 10,
        "col_count": 12
      }
    },
    {
      "id": "REFRAG_elem_000038",
      "type": "table",
      "content": {
        "html": "<table><tr><td></td><td colspan=\"4\">Context Length =4096</td><td colspan=\"4\">Context Length=8192</td><td colspan=\"4\">Context Length=16384</td></tr><tr><td></td><td>Arxiv</td><td>Book</td><td>PG19</td><td>ProofPile</td><td>Arxiv</td><td>Book</td><td>PG19</td><td>ProofPile</td><td>Arxiv</td><td>Book</td><td>PG19</td><td>ProofPile ↓</td></tr><tr><td>LLAMA-FULL CONTEXT</td><td>6.751</td><td>6.956</td><td>6.829</td><td>6.701</td><td>9.675</td><td>9.069</td><td>8.963</td><td>9.401</td><td>9.043</td><td>8.913</td><td>8.848</td><td>8.989</td></tr><tr><td>LLAMA-32K</td><td>1.037</td><td>1.862</td><td>1.960</td><td>0.898</td><td>0.965</td><td>1.867</td><td>1.947</td><td>0.834</td><td>0.865</td><td>1.840</td><td>1.943</td><td>0.770</td></tr><tr><td>LLAMA-No CONTEXT</td><td>1.253</td><td>1.925</td><td>2.030</td><td>1.126</td><td>1.226</td><td>1.949</td><td>2.032</td><td>1.110</td><td>1.174</td><td>1.939</td><td>2.041</td><td>1.081</td></tr><tr><td>REPLUG</td><td>1.253</td><td>1.925</td><td>2.030</td><td>1.126</td><td>1.226</td><td>1.949</td><td>2.032</td><td>1.110</td><td>1.174</td><td>1.939</td><td>2.041</td><td>1.081</td></tr><tr><td>CEPE</td><td>1.085</td><td>1.856</td><td>1.959</td><td>0.945</td><td>1.032</td><td>1.878</td><td>1.958</td><td>0.904</td><td>0.960</td><td>1.864</td><td>1.966</td><td>0.863</td></tr><tr><td>REFRAG8</td><td>1.042</td><td>1.837</td><td>1.922</td><td>0.894</td><td>0.983</td><td>1.839</td><td>1.922</td><td>0.858</td><td>0.977</td><td>1.840</td><td>1.939</td><td>0.891</td></tr><tr><td>REFRAG16</td><td>1.058</td><td>1.847</td><td>1.934</td><td>0.910</td><td>0.994</td><td>1.845</td><td>1.932</td><td>0.871</td><td>0.942</td><td>1.840</td><td>1.945</td><td>0.850</td></tr><tr><td>REFRAG32</td><td>1.088</td><td>1.857</td><td>1.946</td><td>0.944</td><td>1.032</td><td>1.860</td><td>1.945</td><td>0.912</td><td>0.969</td><td>1.852</td><td>1.955</td><td>0.880</td></tr></table>",
        "captions": [
          "Table 2 Log-Perplexity on output tokens ",
          "x _ { s + 1 : s + o }",
          "given different length of context. We use ",
          "s \\in \\{ 4 0 9 6 , 8 1 9 2 , 1 6 3 8 4 \\}",
          "and ",
          "o = 2 0 4 8",
          "here. Bolding are based on comparing baselines excluding LLaMA-Full Context and LLaMA-32K since they are expected to be the best (ideally). The lower the better (↓). "
        ],
        "description": ""
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 5,
        "bbox": {
          "x1": 114,
          "y1": 332,
          "x2": 885,
          "y2": 450
        },
        "section_title": "4.1 Ablation Study ",
        "image_path": "D:/Code_list/Code_Python/Agent_gragh_rag/files/file_store/md_store/minerU_work/REFRAG/images/2126713f80b581f6736151e08d8935600ce900c54f56500fc5af5d09b2301ae7.jpg"
      },
      "metadata": {
        "table_type": "complex_table",
        "row_count": 9,
        "col_count": 12
      }
    },
    {
      "id": "REFRAG_elem_000039",
      "type": "paragraph",
      "content": {
        "text": "performance of the reconstruction task with and without curriculum learning (i.e., reconstruction of $x _ { 1 : s }$from $s / k$chunk embedding directly). The results indicate that curriculum learning is essential for the success of the reconstruction task. "
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 5,
        "bbox": {
          "x1": 109,
          "y1": 474,
          "x2": 887,
          "y2": 518
        },
        "section_title": "4.1 Ablation Study "
      },
      "metadata": {
        "char_count": 258
      }
    },
    {
      "id": "REFRAG_elem_000040",
      "type": "paragraph",
      "content": {
        "text": "Reconstruction task is essential for the model to learn the continual pre-training task. Table 12 shows the performance of the continual pre-training task with and without initialization from the reconstruction task. The results indicate that pre-training on the reconstruction task is important for the success of continual pre-training. "
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 5,
        "bbox": {
          "x1": 109,
          "y1": 527,
          "x2": 887,
          "y2": 587
        },
        "section_title": "4.1 Ablation Study "
      },
      "metadata": {
        "char_count": 339
      }
    },
    {
      "id": "REFRAG_elem_000041",
      "type": "paragraph",
      "content": {
        "text": "Advantages of RL-based selective compression. Figure 3 under various compression rates, achieved by varying the number of chunks to compress (i.e., adjusting $p$). Notably, a compression rate of 8 can be obtained either by configuring REFRAG16 to compress the appropriate number of chunks, or by employing REFRAG $^ { 8 }$with full compression, which is natively trained at a compression rate of 8. This raises a natural question: does the former approach outperform the latter? Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths. "
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 5,
        "bbox": {
          "x1": 109,
          "y1": 595,
          "x2": 887,
          "y2": 686
        },
        "section_title": "4.1 Ablation Study "
      },
      "metadata": {
        "char_count": 631
      }
    },
    {
      "id": "REFRAG_elem_000042",
      "type": "image",
      "content": {
        "captions": [
          "Figure 3 Log-Perplexity on ",
          "x _ { s + 1 : s + o }",
          "under varying compression rates by selectively compressing different percentages of chunks. We compare three selection methods: RL (trained policy), Perplexity-desc (heuristic: lower perplexity), Perplexity-asc (heuristic: higher perplexity), and Random (random selection). "
        ],
        "description": ""
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 5,
        "bbox": {
          "x1": 114,
          "y1": 710,
          "x2": 883,
          "y2": 853
        },
        "section_title": "4.1 Ablation Study ",
        "image_path": "D:/Code_list/Code_Python/Agent_gragh_rag/files/file_store/md_store/minerU_work/REFRAG/images/caa26511083be1c2d8491f6c0d90b86f45d424ae0ec30e112af7645512163b91.jpg"
      }
    },
    {
      "id": "REFRAG_elem_000043",
      "type": "image",
      "content": {
        "captions": [
          "Llama REFRAG 8 × compression "
        ],
        "description": ""
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 6,
        "bbox": {
          "x1": 125,
          "y1": 78,
          "x2": 318,
          "y2": 172
        },
        "section_title": "4.1 Ablation Study ",
        "image_path": "D:/Code_list/Code_Python/Agent_gragh_rag/files/file_store/md_store/minerU_work/REFRAG/images/5ed1e334dd0dd0ee255404c503d13de58df6eb58f1d659bded347401843848bd.jpg"
      }
    },
    {
      "id": "REFRAG_elem_000044",
      "type": "image",
      "content": {
        "captions": [
          "Llama REFRAG 8 × compression ",
          "Figure 4 RAG performance comparison under a strong retriever scenario (left) and a weak retriever scenario and a strong retriever scenario (right). REFRAG perform similarly to LLaMA model under the same retrieved passages (slightly better in a weaker retriever case) while outperform significantly under the same latency. "
        ],
        "description": ""
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 6,
        "bbox": {
          "x1": 689,
          "y1": 79,
          "x2": 870,
          "y2": 172
        },
        "section_title": "4.1 Ablation Study ",
        "image_path": "D:/Code_list/Code_Python/Agent_gragh_rag/files/file_store/md_store/minerU_work/REFRAG/images/120659b02c8920d157a2354370b4b55412bab8f19b9dd8f56d3626c25153c89b.jpg"
      }
    },
    {
      "id": "REFRAG_elem_000045",
      "type": "paragraph",
      "content": {
        "text": "This finding is particularly surprising, as REFRAG16 achieves a compression rate of 8 without recomputing chunk embeddings, yet still surpasses the performance of REFRAG $^ { 8 }$. These results further highlight the effectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression rate without compromising performance. "
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 6,
        "bbox": {
          "x1": 109,
          "y1": 273,
          "x2": 887,
          "y2": 335
        },
        "section_title": "4.1 Ablation Study "
      },
      "metadata": {
        "char_count": 368
      }
    },
    {
      "id": "REFRAG_elem_000046",
      "type": "paragraph",
      "content": {
        "text": "REFRAG trained under different compression rates. Figure 10 illustrates the training trajectory of REFRAG under different compression rates in the continual pre-training task. We observe a performance regression as the compression rate increases; however, even at a compression rate of 32, our model remains competitive (as shown in table 1). In contrast, a compression rate of 64 appears to be overly aggressive, resulting in diminished performance. These findings suggest a practical limit to the compression rate beyond which the model’s capability is significantly reduced. "
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 6,
        "bbox": {
          "x1": 109,
          "y1": 340,
          "x2": 887,
          "y2": 434
        },
        "section_title": "4.1 Ablation Study "
      },
      "metadata": {
        "char_count": 578
      }
    },
    {
      "id": "REFRAG_elem_000047",
      "type": "paragraph",
      "content": {
        "text": "Different combinations of encoder and decoder models for REFRAG. We employ LLaMA-2-7B and LLaMA-2-13B as decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance varies with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder combinations. We observe that increasing the number of parameters in the decoder leads to a substantial reduction in loss, whereas enlarging the encoder yields only a modest improvement. This discrepancy may be attributed to the relatively minor increase in size from RoBERTa-Base to RoBERTa-Large compared to the substantial jump from 7B to 13B in the decoder. Additional results in figure 12 indicate that a larger encoder may not always be advantageous when training with limited data in the continual pre-training setting. This observation aligns with previous findings by Li et al. (2024), which demonstrate that larger encoders in multi-modal models can negatively impact performance when data is scarce. To further validate our training approach on other decoder models, we conduct experiments with LLaMA-3.1-8B and LLaMA-3.2-3B. Table 14 reports the performance of these models paired with RoBERTa-Base and RoBERTa-Large encoders on the Arxiv domain. Models trained with our recipe achieve performance comparable to the Full Context setting (i.e., without context compression). Moreover, increasing the context length continues to benefit our model, as evidenced by lower perplexity for a context length of 4096 compared to 2048. "
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 6,
        "bbox": {
          "x1": 109,
          "y1": 439,
          "x2": 887,
          "y2": 667
        },
        "section_title": "4.1 Ablation Study "
      },
      "metadata": {
        "char_count": 1542
      }
    },
    {
      "id": "REFRAG_elem_000048",
      "type": "title",
      "content": {
        "text": "5 Contextual Learning Applications ",
        "level": 1
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 6,
        "bbox": {
          "x1": 109,
          "y1": 689,
          "x2": 478,
          "y2": 709
        }
      },
      "metadata": {
        "char_count": 35
      }
    },
    {
      "id": "REFRAG_elem_000049",
      "type": "paragraph",
      "content": {
        "text": "In this section, we investigate fine-tuning the model obtained from the pre-training stage to address various downstream tasks, including RAG, long document summarization, and multi-turn conversation with RAG. For each application, we curate an instruction-tuning dataset to facilitate model fine-tuning. "
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 6,
        "bbox": {
          "x1": 109,
          "y1": 720,
          "x2": 887,
          "y2": 767
        },
        "section_title": "5 Contextual Learning Applications "
      },
      "metadata": {
        "char_count": 305
      }
    },
    {
      "id": "REFRAG_elem_000050",
      "type": "title",
      "content": {
        "text": "5.1 Retrieval Augmented Generation ",
        "level": 1
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 6,
        "bbox": {
          "x1": 109,
          "y1": 786,
          "x2": 426,
          "y2": 801
        }
      },
      "metadata": {
        "char_count": 35
      }
    },
    {
      "id": "REFRAG_elem_000051",
      "type": "paragraph",
      "content": {
        "text": "Training dataset. We follow the work of Lin et al. (2024) and use a combination of question answering datasets from 5 domains to fine-tune our model, which contains 1.1 million data points. Dialogue: OpenAssistant Conversations Dataset. Open-Domain QA: CommonsenseQA, MathQA, Web Questions, Wiki Question Answering, Yahoo! Answers QA, FreebaseQA, MS MARCO. Reading Comprehension: Discrete Reasoning Over Paragraphs, PubMedQA, QuaRel, SQuADv2. Chain-of-thought Reasoning: Algebra QA with Rationales, Explanations for CommonsenseQ, Grade School Math 8K, MathQA, StrategyQA. "
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 6,
        "bbox": {
          "x1": 109,
          "y1": 809,
          "x2": 888,
          "y2": 902
        },
        "section_title": "5.1 Retrieval Augmented Generation "
      },
      "metadata": {
        "char_count": 572
      }
    },
    {
      "id": "REFRAG_elem_000052",
      "type": "paragraph",
      "content": {
        "text": "Evaluation dataset. We hold out 5% of the data for each dataset in the training dataset for evaluation. Additionally, we use the datasets that are commonly used in RAG literature (Izacard et al., 2023b; Lin et al., 2024), including MMLU (Hendrycks et al., 2021), BoolQ (Clark et al., 2019), SIQA (Sap et al., 2019), PIQA (Bisk et al., 2020), and Knowledge Intensive Language Tasks (KILT) (Petroni et al., 2020) (including HellaSwag, Winogrande, TQA, FEVER, NQ). We evaluate our performance on 2 settings: 1) Strong Retriever: In this setting we use a strong retriever and retrieve the K-nearest neighbors to answer the question; 2) Weak Retriever: In this setting we retrieve 200 passages and pick random K passages to answer the question. The weak retriever setting closely resembles real-world systems, as RAG retrieval systems often suffer from error accumulation across subsystems. A table summarizing the evaluation metrics for each dataset is included in table 7. "
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 7,
        "bbox": {
          "x1": 109,
          "y1": 80,
          "x2": 888,
          "y2": 232
        },
        "section_title": "5.1 Retrieval Augmented Generation "
      },
      "metadata": {
        "char_count": 970
      }
    },
    {
      "id": "REFRAG_elem_000053",
      "type": "paragraph",
      "content": {
        "text": "Retriever and retrieval corpus. We follow the work of Lin et al. (2024) to use Wikipedia dumps and CommonCrawl dumps to create a retrieval corpus with 400 million passages. Each passage contains less than 200 words. We use the DRAGON+ model Lin et al. (2023) as our retriever and use the implementation of Izacard et al. (2023a) to retrieve the K-nearest neighbors as the retrieved passages for each question. "
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 7,
        "bbox": {
          "x1": 109,
          "y1": 239,
          "x2": 887,
          "y2": 301
        },
        "section_title": "5.1 Retrieval Augmented Generation "
      },
      "metadata": {
        "char_count": 410
      }
    },
    {
      "id": "REFRAG_elem_000054",
      "type": "paragraph",
      "content": {
        "text": "Result analysis. Table 3 shows the performance of different baselines under short and long contexts (i.e., varying number of retrieved passages)3. (1/# tokens) is inverse for the number of tokens in the decoder model. This is used as a metric to gauge the latency of the model (the higher, the lower latency). LLaMAFT is the original LLaMA-2-7B model that is fine-tuned on the same RAG dataset used to train our model. We compare the performance under both the short context and the long context scenarios. For the short context, we use 1 passage for LLaMAFT and use 8 passages for all our models. The baseline of REFRAG8 will have the same latency as the LLaMAFT model. However, due to the compression, we are able to have more context information and hence achieve better performance. Surprisingly, REFRAG $^ { 1 6 }$and REFRAG $^ { 3 2 }$both outperform the LLaMAFT model despite having 2 $\\times$and 4 $\\times$fewer tokens in the decoder (i.e., lower latency). The same result occurs in long context scenarios. Our model has even higher performance gains in multi-choice tasks. Table 15 shows the performance of our model under different numbers of passages. The result suggests that most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged over all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that under the same number of retrieved passages, we are able to match the performance of LLaMA in the strong retriever setting and even outperform LLaMA under the weak retriever setting. This is because our model enables larger context and hence enables extract more useful information when the retrieved passages are less relevant. Under equivalent latency constraints, REFRAG consistently outperform LLaMA on both settings as the saved context can be reinvested to include additional information within the same latency budget. "
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 7,
        "bbox": {
          "x1": 109,
          "y1": 306,
          "x2": 888,
          "y2": 580
        },
        "section_title": "5.1 Retrieval Augmented Generation "
      },
      "metadata": {
        "char_count": 1924
      }
    },
    {
      "id": "REFRAG_elem_000055",
      "type": "paragraph",
      "content": {
        "text": "Figure 4 compares the performance of REFRAG and the LLaMA model under two conditions: 1) an equal number of retrieved passages, and 2) equal latency, for both strong and weak retriever settings. With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26 $\\times$speedup in TTFT. At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22% average improvement across 16 RAG tasks. With a weak retriever setting, at 10 passages, REFRAG improves performance by 0.71% and accelerates TTFT by 5.26 $\\times$compared to LLaMA. At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG achieves a 1.93% average gain over 16 RAG tasks. "
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 7,
        "bbox": {
          "x1": 109,
          "y1": 585,
          "x2": 888,
          "y2": 694
        },
        "section_title": "5.1 Retrieval Augmented Generation "
      },
      "metadata": {
        "char_count": 708
      }
    },
    {
      "id": "REFRAG_elem_000056",
      "type": "title",
      "content": {
        "text": "5.2 Multi-Turn Conversation ",
        "level": 1
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 7,
        "bbox": {
          "x1": 109,
          "y1": 710,
          "x2": 362,
          "y2": 726
        }
      },
      "metadata": {
        "char_count": 28
      }
    },
    {
      "id": "REFRAG_elem_000057",
      "type": "paragraph",
      "content": {
        "text": "We use three different knowledge-intensive multi-turn conversation datasets: TopiOCQA (Adlakha et al., 2022), ORConvQA (Qu et al., 2020), and QReCC (Anantha et al., 2021). For each conversation turn, we retrieve $K$passages using the same retriever and retrieval corpus as described in section 5.1. "
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 7,
        "bbox": {
          "x1": 109,
          "y1": 734,
          "x2": 887,
          "y2": 781
        },
        "section_title": "5.2 Multi-Turn Conversation "
      },
      "metadata": {
        "char_count": 299
      }
    },
    {
      "id": "REFRAG_elem_000058",
      "type": "paragraph",
      "content": {
        "text": "Result analysis. Table 4 presents results across varying numbers of conversational turns and retrieved passages. Our model outperforms LLaMAFT on two out of three datasets in the 5-passage setting, and on all three datasets in the 10-passage setting. This improvement is attributable to the limited 4k-token context window of LLaMAFT, which necessitates truncating portions of the conversational history in longer contexts, resulting in the loss of crucial information required to answer subsequent questions. In contrast, our model, trained on the same LLaMA model without extending its effective positional encoding, maintains robust performance "
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 7,
        "bbox": {
          "x1": 109,
          "y1": 787,
          "x2": 888,
          "y2": 878
        },
        "section_title": "5.2 Multi-Turn Conversation "
      },
      "metadata": {
        "char_count": 648
      }
    },
    {
      "id": "REFRAG_elem_000059",
      "type": "table",
      "content": {
        "html": "<table><tr><td>Generation</td><td>NQ</td><td>FEVER</td><td>TQA</td><td>WebQA</td><td>FreebaseQA</td><td>GSM8K</td><td>StrategyQA</td><td>BoolQ ↑</td><td>(1/# tokens)</td></tr><tr><td colspan=\"10\">Short context with the same latency</td></tr><tr><td>\\( LLAMA_{FT} + 1 \\) passage</td><td>23.96</td><td>62.04</td><td>9.64</td><td>37.33</td><td>75.18</td><td>7.38</td><td>64.44</td><td>29.24</td><td>1×</td></tr><tr><td>\\( REFRAG_{8} + 8 \\) passages</td><td>22.96</td><td>66.59</td><td>13.05</td><td>38.67</td><td>73.46</td><td>7.38</td><td>75.56</td><td>3.30</td><td>1×</td></tr><tr><td>\\( REFRAG_{16} + 8 \\) passages</td><td>22.94</td><td>62.88</td><td>12.97</td><td>42.67</td><td>71.50</td><td>9.40</td><td>71.11</td><td>5.87</td><td>2×</td></tr><tr><td>\\( REFRAG_{32} + 8 \\) passages</td><td>22.11</td><td>64.24</td><td>12.57</td><td>41.33</td><td>71.74</td><td>12.75</td><td>73.33</td><td>1.99</td><td>4×</td></tr><tr><td colspan=\"10\">Long context</td></tr><tr><td>\\( LLAMA_{FT} + 10 \\) passages</td><td>26.08</td><td>65.44</td><td>9.68</td><td>40.00</td><td>76.17</td><td>6.71</td><td>68.89</td><td>30.00</td><td>1×</td></tr><tr><td>CEPED +80 passages</td><td>0.03</td><td>65.68</td><td>0.01</td><td>0.00</td><td>0.00</td><td>0.00</td><td>0.00</td><td>57.80</td><td></td></tr><tr><td>REPLUG +80 passages</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>64.44</td><td>-</td><td></td></tr><tr><td>\\( LLAMA-32K +80 \\) passages</td><td>1.24</td><td>0.14</td><td>0.52</td><td>10.67</td><td>9.83</td><td>0.00</td><td>0.00</td><td>0.03</td><td></td></tr><tr><td>\\( REFRAG_{8} +80 \\) passages</td><td>24.15</td><td>68.83</td><td>13.06</td><td>37.33</td><td>74.20</td><td>7.38</td><td>71.11</td><td>7.03</td><td>1×</td></tr><tr><td>\\( REFRAG_{16} +80 \\) passages</td><td>23.30</td><td>66.01</td><td>12.65</td><td>38.67</td><td>75.43</td><td>12.08</td><td>73.33</td><td>12.23</td><td>2×</td></tr><tr><td>\\( REFRAG_{32} +80 \\) passages</td><td>23.02</td><td>68.48</td><td>12.14</td><td>38.67</td><td>71.74</td><td>9.40</td><td>68.89</td><td>6.42</td><td>4×</td></tr><tr><td>Multi-Choice</td><td>MMLU</td><td>CommonsenseQA</td><td>MathQA</td><td>ECQA</td><td>HellaSwag</td><td>SIQA</td><td>PIQA</td><td>Winogrande ↑</td><td></td></tr><tr><td colspan=\"10\">Short context with the same latency</td></tr><tr><td>\\( LLAMA_{FT} + 1 \\) context</td><td>50.23</td><td>85.05</td><td>99.50</td><td>84.77</td><td>41.80</td><td>68.12</td><td>67.36</td><td>55.64</td><td>1×</td></tr><tr><td>\\( REFRAG_{8} + 8 \\) passages</td><td>50.29</td><td>92.27</td><td>99.66</td><td>94.70</td><td>45.23</td><td>68.94</td><td>71.38</td><td>57.70</td><td>1×</td></tr><tr><td>\\( REFRAG_{16} + 8 \\) passages</td><td>49.84</td><td>89.18</td><td>99.66</td><td>98.01</td><td>39.33</td><td>68.42</td><td>70.29</td><td>56.67</td><td>2×</td></tr><tr><td>\\( REFRAG_{32} + 8 \\) passages</td><td>49.51</td><td>91.75</td><td>99.50</td><td>97.35</td><td>42.86</td><td>68.17</td><td>68.34</td><td>56.75</td><td>4×</td></tr><tr><td colspan=\"10\">Long context</td></tr><tr><td>\\( LLAMA_{FT} + 10 \\) passages</td><td>48.66</td><td>82.99</td><td>68.46</td><td>84.11</td><td>41.77</td><td>67.45</td><td>68.01</td><td>53.91</td><td>1×</td></tr><tr><td>CEPED +80 passages</td><td>26.26</td><td>26.29</td><td>23.66</td><td>24.50</td><td>24.95</td><td>32.86</td><td>48.53</td><td>44.51</td><td></td></tr><tr><td>REPLUG +80 passages</td><td>-</td><td>78.35</td><td>-</td><td>76.16</td><td>-</td><td>65.51</td><td>-</td><td>-</td><td></td></tr><tr><td>\\( LLAMA-32K +80 \\) passages</td><td>22.21</td><td>16.49</td><td>19.80</td><td>16.56</td><td>23.76</td><td>24.16</td><td>34.17</td><td>48.86</td><td></td></tr><tr><td>\\( REFRAG_{8} +80 \\) passages</td><td>50.42</td><td>92.27</td><td>99.66</td><td>97.35</td><td>44.61</td><td>68.22</td><td>69.37</td><td>57.54</td><td>1×</td></tr><tr><td>\\( REFRAG_{16} +80 \\) passages</td><td>50.88</td><td>89.69</td><td>99.66</td><td>96.69</td><td>38.50</td><td>68.47</td><td>70.89</td><td>56.99</td><td>2×</td></tr><tr><td>\\( REFRAG_{32} +80 \\) passages</td><td>49.77</td><td>90.72</td><td>99.50</td><td>98.01</td><td>43.37</td><td>68.47</td><td>69.04</td><td>56.99</td><td>4×</td></tr></table>",
        "captions": [
          "Table 3 Comparison of model performance of different models with different number of retrieved passages for RAG under the strong retriever scenario. "
        ],
        "description": ""
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 8,
        "bbox": {
          "x1": 114,
          "y1": 116,
          "x2": 883,
          "y2": 404
        },
        "section_title": "5.2 Multi-Turn Conversation ",
        "image_path": "D:/Code_list/Code_Python/Agent_gragh_rag/files/file_store/md_store/minerU_work/REFRAG/images/300b99a8528e98a1b98d6493c929e3e254c122ce013292eeeb757c5d032a9221.jpg"
      },
      "metadata": {
        "table_type": "complex_table",
        "row_count": 27,
        "col_count": 8
      }
    },
    {
      "id": "REFRAG_elem_000060",
      "type": "paragraph",
      "content": {
        "text": "even with a large number of passages, owing to the benefits of our compression approach. Table 5 further reports the performance of different models under varying numbers of passages, with our model consistently achieving superior results on two out of three datasets for the reasons outlined above. "
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 8,
        "bbox": {
          "x1": 109,
          "y1": 436,
          "x2": 883,
          "y2": 483
        },
        "section_title": "5.2 Multi-Turn Conversation "
      },
      "metadata": {
        "char_count": 300
      }
    },
    {
      "id": "REFRAG_elem_000061",
      "type": "table",
      "content": {
        "html": "<table><tr><td></td><td># Turns (≥)</td><td>ORConvQA</td><td>QReCC</td><td>TopiOCQA ↑</td></tr><tr><td></td><td colspan=\"4\"># Passages = 5</td></tr><tr><td>\\( LLAMA_{FT} \\)</td><td>2</td><td>20.73</td><td>18.72</td><td>26.98</td></tr><tr><td>\\( REFRAG_{8} \\)</td><td>2</td><td>21.17</td><td>17.73</td><td>28.04</td></tr><tr><td>\\( REFRAG_{16} \\)</td><td>2</td><td>20.19</td><td>17.30</td><td>27.89</td></tr><tr><td>\\( REFRAG_{32} \\)</td><td>2</td><td>19.70</td><td>17.35</td><td>28.67</td></tr><tr><td>\\( LLAMA_{FT} \\)</td><td>4</td><td>20.33</td><td>16.42</td><td>23.50</td></tr><tr><td>\\( REFRAG_{8} \\)</td><td>4</td><td>22.78</td><td>15.61</td><td>26.93</td></tr><tr><td>\\( REFRAG_{16} \\)</td><td>4</td><td>21.94</td><td>15.27</td><td>27.03</td></tr><tr><td>\\( REFRAG_{32} \\)</td><td>4</td><td>21.68</td><td>15.45</td><td>26.45</td></tr><tr><td>\\( LLAMA_{FT} \\)</td><td>6</td><td>20.76</td><td>11.92</td><td>23.10</td></tr><tr><td>\\( REFRAG_{8} \\)</td><td>6</td><td>23.11</td><td>10.88</td><td>25.37</td></tr><tr><td>\\( REFRAG_{16} \\)</td><td>6</td><td>21.69</td><td>10.75</td><td>26.17</td></tr><tr><td>\\( REFRAG_{32} \\)</td><td>6</td><td>21.19</td><td>10.69</td><td>25.51</td></tr></table>",
        "captions": [
          "Table 4 Performance on multi-turn RAG tasks for # Passages = 5 and # Passages = 10. "
        ],
        "description": ""
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 8,
        "bbox": {
          "x1": 114,
          "y1": 518,
          "x2": 493,
          "y2": 676
        },
        "section_title": "5.2 Multi-Turn Conversation ",
        "image_path": "D:/Code_list/Code_Python/Agent_gragh_rag/files/file_store/md_store/minerU_work/REFRAG/images/b2853c8d93458f21afe49843980bc511868409703ae23a014b2910820c44d1c5.jpg"
      },
      "metadata": {
        "table_type": "complex_table",
        "row_count": 13,
        "col_count": 4
      }
    },
    {
      "id": "REFRAG_elem_000062",
      "type": "table",
      "content": {
        "html": "<table><tr><td></td><td># Turns (≥)</td><td>ORConvQA</td><td>QReCC</td><td>TopiOCQA ↑</td></tr><tr><td></td><td colspan=\"4\"># Passages = 10</td></tr><tr><td>\\( LLAMA_{FT} \\)</td><td>2</td><td>16.52</td><td>17.31</td><td>23.02</td></tr><tr><td>\\( REFRAG_{8} \\)</td><td>2</td><td>21.15</td><td>17.92</td><td>27.97</td></tr><tr><td>\\( REFRAG_{16} \\)</td><td>2</td><td>20.79</td><td>17.37</td><td>28.45</td></tr><tr><td>\\( REFRAG_{32} \\)</td><td>2</td><td>19.67</td><td>17.16</td><td>28.31</td></tr><tr><td>\\( LLAMA_{FT} \\)</td><td>4</td><td>16.90</td><td>14.69</td><td>20.23</td></tr><tr><td>\\( REFRAG_{8} \\)</td><td>4</td><td>22.63</td><td>15.68</td><td>25.95</td></tr><tr><td>\\( REFRAG_{16} \\)</td><td>4</td><td>21.84</td><td>15.21</td><td>26.12</td></tr><tr><td>\\( REFRAG_{32} \\)</td><td>4</td><td>21.75</td><td>15.33</td><td>25.77</td></tr><tr><td>\\( LLAMA_{FT} \\)</td><td>6</td><td>14.44</td><td>10.72</td><td>19.52</td></tr><tr><td>\\( REFRAG_{8} \\)</td><td>6</td><td>20.59</td><td>11.00</td><td>25.16</td></tr><tr><td>\\( REFRAG_{16} \\)</td><td>6</td><td>21.05</td><td>10.50</td><td>24.96</td></tr><tr><td>\\( REFRAG_{32} \\)</td><td>6</td><td>21.67</td><td>10.79</td><td>25.23</td></tr></table>",
        "captions": [],
        "description": ""
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 8,
        "bbox": {
          "x1": 506,
          "y1": 518,
          "x2": 885,
          "y2": 675
        },
        "section_title": "5.2 Multi-Turn Conversation ",
        "image_path": "D:/Code_list/Code_Python/Agent_gragh_rag/files/file_store/md_store/minerU_work/REFRAG/images/7d8b12cbd985facd1b3470b4a91410f4cc51f1040ee9322e5f48b13e01a0ae18.jpg"
      },
      "metadata": {
        "table_type": "complex_table",
        "row_count": 13,
        "col_count": 4
      }
    },
    {
      "id": "REFRAG_elem_000063",
      "type": "table",
      "content": {
        "html": "<table><tr><td colspan=\"4\">REFRAG</td><td colspan=\"3\">LLAMAFT</td></tr><tr><td># Passages</td><td>ORConvQA</td><td>QReCC</td><td>TopiOCQA ↑</td><td>ORConvQA</td><td>QReCC</td><td>TopiOCQA ↑</td></tr><tr><td>0</td><td>19.27</td><td>15.32</td><td>28.19</td><td>19.16</td><td>15.49</td><td>28.22</td></tr><tr><td>5</td><td>20.18</td><td>17.37</td><td>28.24</td><td>19.65</td><td>18.71</td><td>27.08</td></tr><tr><td>8</td><td>20.52</td><td>17.60</td><td>28.17</td><td>16.87</td><td>18.05</td><td>25.36</td></tr><tr><td>10</td><td>19.67</td><td>17.41</td><td>27.62</td><td>15.72</td><td>17.42</td><td>23.60</td></tr></table>",
        "captions": [
          "Table 5 Performance on multi-turn RAG tasks with different number of passages. "
        ],
        "description": ""
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 8,
        "bbox": {
          "x1": 230,
          "y1": 717,
          "x2": 771,
          "y2": 792
        },
        "section_title": "5.2 Multi-Turn Conversation ",
        "image_path": "D:/Code_list/Code_Python/Agent_gragh_rag/files/file_store/md_store/minerU_work/REFRAG/images/04f31501fb6a6b74a0bda78a9616fd9ec1fdcd157631ba54b519c095408b77cb.jpg"
      },
      "metadata": {
        "table_type": "complex_table",
        "row_count": 5,
        "col_count": 6
      }
    },
    {
      "id": "REFRAG_elem_000064",
      "type": "title",
      "content": {
        "text": "6 Related Works ",
        "level": 1
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 8,
        "bbox": {
          "x1": 109,
          "y1": 820,
          "x2": 294,
          "y2": 838
        }
      },
      "metadata": {
        "char_count": 16
      }
    },
    {
      "id": "REFRAG_elem_000065",
      "type": "paragraph",
      "content": {
        "text": "Retrieval-Augmented Language Modeling. Recent research has extensively investigated novel model architectures to improve retrieval-augmented generation. Guu et al. (2020) introduced pre-training for retrievalaugmented masked language models. Building on this, Borgeaud et al. (2022) proposed a new architecture and pre-training paradigm for generative LLMs, leveraging cross-attention and end-to-end pre-training with retrieval from a trillion-token data store, achieving strong performance. Subsequent work by Shi et al. (2024) and Lin et al. (2024) focused on fine-tuning existing LLMs by prepending retrieved passages to prompts and employing ensemble methods for response generation. Additionally, Izacard and Grave (2021) introduced fusion-in-decoder, which uses an encoder to process each passage in parallel and concatenates the hidden states for generation via a decoder. This approach accelerates attention computation by removing cross-document attention, but does not apply compression in the decoder, which could further reduce latency. "
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 8,
        "bbox": {
          "x1": 109,
          "y1": 852,
          "x2": 885,
          "y2": 914
        },
        "section_title": "6 Related Works "
      },
      "metadata": {
        "char_count": 1049
      }
    },
    {
      "id": "REFRAG_elem_000066",
      "type": "paragraph",
      "content": {
        "text": "Efficient Long-Context LLMs. Recent research has investigated various strategies to reduce memory usage and accelerate latency in long-context generation for LLMs. Choromanski et al. (2021) introduced compressed attention, reducing attention complexity from quadratic to linear; however, this method does not address memory requirements. It is complementary to our approach and can be integrated to further improve latency. StreamingLLM(Xiao et al., 2024) proposed attention sinks to decrease KV cache memory for long-context generation, though this does not reduce latency during the pre-filling stage. CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache memory and attention computations. However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE does not utilize token compression, resulting in similar or even increased decoding latency. Concurrently with our work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past context into compact vectors, enabling retrieval of salient information during subsequent processing. Like CEPE, PCC is limited to prefix context applications and does not support arbitrary folding or expansion of contexts at any position. Interestingly, Kuratov et al. (2025) investigated the capacity of LLMs to encode long contexts into a single embedding, demonstrating minimal information loss for sequences up to 1500 tokens. Their work examines the extent to which information can be compressed into a single embedding, offering a complementary perspective to REFRAG, which is designed for decoding from multiple compact embeddings within the standard decoder architecture. "
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 9,
        "bbox": {
          "x1": 112,
          "y1": 188,
          "x2": 888,
          "y2": 462
        },
        "section_title": "6 Related Works "
      },
      "metadata": {
        "char_count": 1842
      }
    },
    {
      "id": "REFRAG_elem_000067",
      "type": "paragraph",
      "content": {
        "text": "Compressive transformer. Rae et al. (2020) first introduced the compressive transformer, which compresses the KV cache to reduce memory requirements for long-context applications. However, this approach only decreases KV cache memory usage, does not improve time-to-first-token latency, and requires training the model from scratch. Yoshida et al. (2021) extended this idea by employing recursive context compression, generating a summary hidden state for each chunk to inform the next chunk’s computation. The recursive nature, however, prevents pre-computation and reuse of chunk embeddings, and does not reduce decoding latency. Chevalier et al. (2023) proposed recursive compression for documents, using compressed embeddings for prediction, similar to our method. However, their sequential compression process results in high latency when the summary vector is not cached, and their approach only supports applications where the summary token is restricted to the prefix of the language model (e.g., RAG), limiting applicability. In contrast, our work is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within the prompt, supporting diverse applications such as RAG and multi-turn conversation. Furthermore, our method learns where to apply compression, allowing for adaptive compression rates at inference time without recomputing chunk embeddings. "
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 9,
        "bbox": {
          "x1": 109,
          "y1": 478,
          "x2": 888,
          "y2": 691
        },
        "section_title": "6 Related Works "
      },
      "metadata": {
        "char_count": 1400
      }
    },
    {
      "id": "REFRAG_elem_000068",
      "type": "paragraph",
      "content": {
        "text": "Prompt compression. Prompt compression seeks to reduce input token length to lower latency and cost while maintaining task performance. A prominent approach is LLMLingua(Jiang et al., 2023),which employs coarse-to-fine, budget-controlled compression with token-level iterative refinement, achieving high compression ratios with minimal performance loss. LongLLMLingua (Jiang et al., 2024) extends this method to long-context scenarios, demonstrating significant cost and end-to-end speed improvements. Complementary approaches rank or prune context by estimated informativeness, e.g., Selective Context uses self-information to drop low-value tokens, and sentence-level methods learn context-aware encoders for question-specific compression and faster inference Li et al. (2023); Liskavets et al. (2024). These approaches are complementary to our work and can be integrated to further reduce the latency of REFRAG. "
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 9,
        "bbox": {
          "x1": 109,
          "y1": 707,
          "x2": 887,
          "y2": 844
        },
        "section_title": "6 Related Works "
      },
      "metadata": {
        "char_count": 915
      }
    },
    {
      "id": "REFRAG_elem_000069",
      "type": "title",
      "content": {
        "text": "7 Conclusion ",
        "level": 1
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 10,
        "bbox": {
          "x1": 112,
          "y1": 80,
          "x2": 256,
          "y2": 97
        }
      },
      "metadata": {
        "char_count": 13
      }
    },
    {
      "id": "REFRAG_elem_000070",
      "type": "paragraph",
      "content": {
        "text": "In this work, we introduced REFRAG, a novel and efficient decoding framework tailored for RAG applications. By leveraging the inherent sparsity and block-diagonal attention patterns present in RAG contexts, REFRAG compresses, senses, and expands context representations to significantly reduce both memory usage and inference latency, particularly the TTFT. Extensive experiments across a range of long-context applications, including RAG, multi-turn conversations, and long document summarization, demonstrate that REFRAG achieves up to 30.85 $\\times$TTFT acceleration (3.75 $\\times$over previous state-of-the-art methods) without any loss in perplexity or downstream accuracy. Our results highlight the importance of specialized treatment for RAGbased systems and open new directions for efficient large-context LLM inference. We believe that REFRAG provides a practical and scalable solution for deploying LLMs in latency-sensitive, knowledge-intensive applications. "
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 10,
        "bbox": {
          "x1": 112,
          "y1": 112,
          "x2": 883,
          "y2": 263
        },
        "section_title": "7 Conclusion "
      },
      "metadata": {
        "char_count": 970
      }
    },
    {
      "id": "REFRAG_elem_000071",
      "type": "title",
      "content": {
        "text": "8 Acknowledgements ",
        "level": 1
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 10,
        "bbox": {
          "x1": 112,
          "y1": 287,
          "x2": 343,
          "y2": 306
        }
      },
      "metadata": {
        "char_count": 19
      }
    },
    {
      "id": "REFRAG_elem_000072",
      "type": "paragraph",
      "content": {
        "text": "We thank for Jason Chen, Yao Liu, Norman Huang, Xueyuan Su, Pranesh Srinivasan, Avinash Atreya, Riham Mansour, Jeremy Teboul for insightful discussions and support. "
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 10,
        "bbox": {
          "x1": 112,
          "y1": 319,
          "x2": 883,
          "y2": 349
        },
        "section_title": "8 Acknowledgements "
      },
      "metadata": {
        "char_count": 165
      }
    },
    {
      "id": "REFRAG_elem_000073",
      "type": "title",
      "content": {
        "text": "References ",
        "level": 1
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 11,
        "bbox": {
          "x1": 112,
          "y1": 80,
          "x2": 227,
          "y2": 97
        }
      },
      "metadata": {
        "char_count": 11
      }
    },
    {
      "id": "REFRAG_elem_000074",
      "type": "list",
      "content": {
        "text": "Vaibhav Adlakha, Shehzaad Dhuliawala, Kaheer Suleman, Harm de Vries, and Siva Reddy. TopiOCQA: Open-domain conversational question answering with topic switching. Transactions of the Association for Computational Linguistics, 10:468–483, 04 2022. ISSN 2307-387X. doi: 10.1162/tacl_a_00471. https://doi.org/10.1162/tacl_a_00471. Raviteja Anantha, Svitlana Vakulenko, Zhucheng Tu, Shayne Longpre, Stephen Pulman, and Srinivas Chappidi. Open-domain question answering goes conversational via question rewriting. Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 2021. Zhangir Azerbayev, Edward Ayers, and Bartosz Piotrowski. Proofpile: A pre-training dataset of mathematical texts. https://huggingface.co/datasets/hoskinson-center/proof-pile, 2023. Dataset available on Hugging Face. The dataset is 13GB and contains 8.3 billion tokens of informal and formal mathematics from diverse sources including arXiv.math, formal math libraries (Lean, Isabelle, Coq, HOL Light, Metamath, Mizar), Math Stack Exchange, Wikipedia math articles, and more. Irwan Bello, Hieu Pham, Quoc V. Le, Mohammad Norouzi, and Samy Bengio. Neural combinatorial optimization with reinforcement learning. In Workshop track of the International Conference on Learning Representations (ICLR), 2017. Iz Beltagy, Matthew E. Peters, and Arman Cohan. Longformer: The long-document transformer. arXiv:2004.05150, 2020. Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. Piqa: Reasoning about physical commonsense in natural language. In Thirty-Fourth AAAI Conference on Artificial Intelligence, 2020. Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, Diego De Las Casas, Aurelia Guy, Jacob Menick, Roman Ring, Tom Hennigan, Saffron Huang, Loren Maggiore, Chris Jones, Albin Cassirer, Andy Brock, Michela Paganini, Geoffrey Irving, Oriol Vinyals, Simon Osindero, Karen Simonyan, Jack Rae, Erich Elsen, and Laurent Sifre. Improving language models by retrieving from trillions of tokens. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 2206–2240. PMLR, 17–23 Jul 2022. https://proceedings.mlr.press/v162/borgeaud22a.html. Alexis Chevalier, Alexander Wettig, Anirudh Ajith, and Danqi Chen. Adapting language models to compress contexts. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 3829–3846, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.232. https://aclanthology.org/2023.emnlp-main.232. Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509, 2019. Krzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin, Lukasz Kaiser, David Benjamin Belanger, Lucy J Colwell, and Adrian Weller. Rethinking attention with performers. In International Conference on Learning Representations, 2021. https://openreview.net/forum?id=Ua6zuk0WRH. Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions. In NAACL, 2019. Arman Cohan, Franck Dernoncourt, Doo Soon Kim, Trung Bui, Seokhwan Kim, Walter Chang, and Nazli Goharian. A discourse-aware attention model for abstractive summarization of long documents. In Marilyn Walker, Heng Ji, and Amanda Stent, editors, Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 615–621, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-2097. https: //aclanthology.org/N18-2097/. Hanjun Dai, Elias B. Khalil, Yuyu Zhang, Bistra Dilkina, and Le Song. Learning combinatorial optimization algorithms over graphs. In Advances in Neural Information Processing Systems (NeurIPS) 30, pages 6348–6358, 2017. Yuhong Dai, Jianxun Lian, Yitian Huang, Wei Zhang, Mingyang Zhou, Mingqi Wu, Xing Xie, and Hao Liao. Pretraining context compressor for large language models with embedding-based memory. In Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar, editors, Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 28715–28732, Vienna, Austria, July "
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 11,
        "bbox": {
          "x1": 111,
          "y1": 112,
          "x2": 887,
          "y2": 904
        },
        "section_title": "References "
      },
      "metadata": {
        "char_count": 4885
      }
    },
    {
      "id": "REFRAG_elem_000075",
      "type": "list",
      "content": {
        "text": "2025. Association for Computational Linguistics. ISBN 979-8-89176-251-0. doi: 10.18653/v1/2025.acl-long.1394. https://aclanthology.org/2025.acl-long.1394/. Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. Retrieval augmented language model pre-training. In International conference on machine learning, pages 3929–3938. PMLR, 2020. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. In Proc. ICLR, 2021. Gautier Izacard and Edouard Grave. Leveraging passage retrieval with generative models for open domain question answering. In Paola Merlo, Jorg Tiedemann, and Reut Tsarfaty, editors, Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 874–880, Online, April 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.eacl-main.74. https://aclanthology.org/ 2021.eacl-main.74/. Gautier Izacard, Mostafa Dehghani, Sina Hosseini, Holger Schwenk, Fabio Petroni, and Sebastian Riedel. Few-shot learning with retrieval augmented language models. arXiv preprint arXiv:2208.03299, 2022. https://arxiv.org/abs/ 2208.03299. Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Edouard Grave, and Sebastian Riedel. Atlas: Few-shot learning with retrieval augmented language models. J. Mach. Learn. Res., 24:37:1–37:37, 2023a. Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. Atlas: Few-shot learning with retrieval augmented language models. Journal of Machine Learning Research, 24(251):1–43, 2023b. Huiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. Llmlingua: Compressing prompts for accelerated inference of large language models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 13358–13376, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.825. https://aclanthology.org/2023.emnlp-main.825/. Huiqiang Jiang, Qianhui Wu, Xufang Luo, Dongsheng Li, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. LongLLMLingua: Accelerating and enhancing LLMs in long context scenarios via prompt compression. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Bangkok, Thailand, August 2024. Association for Computational Linguistics. Yuri Kuratov, Mikhail Arkhipov, Aydar Bulatov, and Mikhail Burtsev. Cramming 1568 tokens into a single vector and back again: Exploring the limits of embedding space capacity. In Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar, editors, Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 19323–19339, Vienna, Austria, July 2025. Association for Computational Linguistics. ISBN 979-8-89176-251-0. doi: 10.18653/v1/2025.acl-long.948. https://aclanthology. org/2025.acl-long.948/. Bozhou Li, Hao Liang, Zimo Meng, and Wentao Zhang. Are bigger encoders always better in vision large models? arXiv preprint arXiv:2408.00620, August 2024. Preprint. Yucheng Li, Bo Dong, Frank Guerin, and Chenghua Lin. Compressing context to enhance inference efficiency of large language models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 6342–6353, Singapore, December 2023. Association for Computational Linguistics. https://aclanthology.org/ 2023.emnlp-main.391.pdf. Sheng-Chieh Lin, Akari Asai, Minghan Li, Barlas Oguz, Jimmy Lin, Yashar Mehdad, Wen tau Yih, and Xilun Chen. How to train your dragon: Diverse augmentation towards generalizable dense retrieval. In The 2023 Conference on Empirical Methods in Natural Language Processing, 2023. https://openreview.net/forum?id=d00kbjbYv2. Xi Victoria Lin, Xilun Chen, Mingda Chen, Weijia Shi, Maria Lomeli, Richard James, Pedro Rodriguez, Jacob Kahn, Gergely Szilvasy, Mike Lewis, Luke Zettlemoyer, and Wen tau Yih. RA-DIT: Retrieval-augmented dual instruction tuning. In The Twelfth International Conference on Learning Representations, 2024. https://openreview.net/ forum?id=22OTbutug9. Barys Liskavets, Maxim Ushakov, Shuvendu Roy, Mark Klibanov, Ali Etemad, and Shane Luke. Prompt compression with context-aware sentence encoding for fast and improved llm inference. arXiv preprint arXiv:2409.01227, 2024. https://arxiv.org/abs/2409.01227. Accepted at AAAI 2025. "
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 12,
        "bbox": {
          "x1": 112,
          "y1": 80,
          "x2": 883,
          "y2": 878
        },
        "section_title": "References "
      },
      "metadata": {
        "char_count": 4634
      }
    },
    {
      "id": "REFRAG_elem_000076",
      "type": "list",
      "content": {
        "text": "Jingyu Liu, Beidi Chen, and Ce Zhang. Speculative prefill: Turbocharging TTFT with lightweight and trainingfree token importance estimation. In Forty-second International Conference on Machine Learning, 2025. https: //openreview.net/forum?id=bzbuZ0ItBq. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations (ICLR), 2019. Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis, Majid Yazdani, Nicola De Cao, James Thorne, Yacine Jernite, Vladimir Karpukhin, Jean Maillard, et al. Kilt: a benchmark for knowledge intensive language tasks. arXiv preprint arXiv:2009.02252, 2020. Chen Qu, Liu Yang, Cen Chen, Minghui Qiu, W. Bruce Croft, and Mohit Iyyer. Open-Retrieval Conversational Question Answering. In SIGIR, 2020. Jack W Rae, Anna Potapenko, Siddhant M Jayakumar, Chloe Hillier, and Timothy P Lillicrap. Compressive transformers for long-range sequence modelling. arXiv preprint, 2019. https://arxiv.org/abs/1911.05507. Jack W. Rae, Anna Potapenko, Siddhant M. Jayakumar, Chloe Hillier, and Timothy P. Lillicrap. Compressive transformers for long-range sequence modelling. In International Conference on Learning Representations, 2020. https://openreview.net/forum?id=SylKikSYDH. Stephen Roller, Emily Dinan, Naman Goyal, Da Ju, Mary Williamson, Yinhan Liu, Jing Xu, Myle Ott, Eric Michael Smith, Y-Lan Boureau, and Jason Weston. Recipes for building an open-domain chatbot. In Paola Merlo, Jorg Tiedemann, and Reut Tsarfaty, editors, Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 300–325, Online, April 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.eacl-main.24. https://aclanthology.org/2021.eacl-main.24/. Maarten Sap, Hannah Rashkin, Derek Chen, Ronan Le Bras, and Yejin Choi. Social IQa: Commonsense reasoning about social interactions. In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan, editors, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 4463–4473, Hong Kong, China, November 2019. Association for Computational Linguistics. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Richard James, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. REPLUG: Retrieval-augmented black-box language models. In Kevin Duh, Helena Gomez, and Steven Bethard, editors, Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 8371–8384, Mexico City, Mexico, June 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.naacl-long.463. https: //aclanthology.org/2024.naacl-long.463/. Xiaoxiang Shi, Colin Cai, and Junjia Du. Proactive intra-gpu disaggregation of prefill and decode in llm serving, 2025. https://arxiv.org/abs/2507.06608. Daria Soboleva, Faisal Al-Khateeb, Robert Myers, Jacob R Steeves, Joel Hestness, and Nolan Dey. SlimPajama: A 627B token cleaned and deduplicated version of RedPajama. https://www.cerebras.net/blog/ slimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama, June 2023. https://huggingface.co/ datasets/cerebras/SlimPajama-627B. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks. In The Twelfth International Conference on Learning Representations, 2024. https://openreview. net/forum?id=NG7sS51zVF. "
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 13,
        "bbox": {
          "x1": 109,
          "y1": 80,
          "x2": 887,
          "y2": 900
        },
        "section_title": "References "
      },
      "metadata": {
        "char_count": 4481
      }
    },
    {
      "id": "REFRAG_elem_000077",
      "type": "list",
      "content": {
        "text": "Howard Yen, Tianyu Gao, and Danqi Chen. Long-context language modeling with parallel context encoding. In Association for Computational Linguistics (ACL), 2024. Davis Yoshida, Allyson Ettinger, and Kevin Gimpel. Adding recurrence to pretrained transformers, 2021. https: //openreview.net/forum?id=taQNxF9Sj6. Yizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, Xiang Gao, Jianfeng Gao, Jingjing Liu, and Bill Dolan. DIALOGPT : Large-scale generative pre-training for conversational response generation. In Asli Celikyilmaz and Tsung-Hsien Wen, editors, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 270–278, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.aclâĂŚdemos.30. https://aclanthology.org/2020.aclâĂŚdemos.30/. "
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 14,
        "bbox": {
          "x1": 112,
          "y1": 80,
          "x2": 885,
          "y2": 222
        },
        "section_title": "References "
      },
      "metadata": {
        "char_count": 844
      }
    },
    {
      "id": "REFRAG_elem_000078",
      "type": "title",
      "content": {
        "text": "Appendix ",
        "level": 1
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 15,
        "bbox": {
          "x1": 109,
          "y1": 75,
          "x2": 250,
          "y2": 101
        }
      },
      "metadata": {
        "char_count": 9
      }
    },
    {
      "id": "REFRAG_elem_000079",
      "type": "title",
      "content": {
        "text": "A Additional Discussion ",
        "level": 1
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 15,
        "bbox": {
          "x1": 109,
          "y1": 122,
          "x2": 362,
          "y2": 138
        }
      },
      "metadata": {
        "char_count": 24
      }
    },
    {
      "id": "REFRAG_elem_000080",
      "type": "paragraph",
      "content": {
        "text": "Analysis on latency and throughput improvement. We denote the following parameters: $s$as the context length, $o$as the output length, $b$as the batch size, $d$as the dimensionality of the hidden states, $\\it { \\Delta } l$as the number of layers in the decoder, and $n$as the number of model parameters. The flop rate of the GPU is $f$, and the high bandwidth memory of the GPU is $m$and we use the compression rate of $k$in our encoder. We assume that all our chunk embeddings are precomputed and cached. The model is loaded with bfloat16 precision. We focus our analysis on LLaMA-2-7B model. The results should be generalizable to other models. We use the following metrics: TTFT which is the latency for the system to generate the first token; TTIT which is the time that it takes to generate iterative token after the first token; Throughput which is the number of tokens that are generated from the system in a unit time. Table 6 shows that with short context length s we are able to achieve $k \\times$acceleration in TTFT and up to $k \\times$acceleration in throughput. With longer context length $s$, we are able to achieve up to $k ^ { 2 } \\times$acceleration in both TTFT and throughput. The details on the latency and throughput calculation are in section B.4. "
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 15,
        "bbox": {
          "x1": 109,
          "y1": 154,
          "x2": 887,
          "y2": 335
        },
        "section_title": "A Additional Discussion "
      },
      "metadata": {
        "char_count": 1271
      }
    },
    {
      "id": "REFRAG_elem_000081",
      "type": "paragraph",
      "content": {
        "text": "Empirical verification of latency/throughput improvement. Figure 2 shows the empirical measurement of the acceleration of REFRAG compared with CEPE, a previous work that achieves significant acceleration in inference (Yen et al., 2024). Under the context length of 16384 (i.e., mid-to-long context), REFRAG achieves $1 6 . 5 3 \\times$acceleration in TTFT with cache and 8.59× without cache. Both higher than CEPE (i.e., 2.01 $\\times$and $1 . 0 4 \\times$acceleration respectively) while having better model performance (see table 1). With longer context, we are able to achieve up to $3 2 . 9 9 \\times$acceleration in TTFT. The reason why we get such acceleration even without cache is that the encoder is light-weight (e.g., Roberta-large is 355M-sized) and the chunks are processed parallel without attending to each other. In terms of TTIT, we achieve $3 \\times$acceleration in long context scenario in both cached and not cached scenarios. This is expected since they have the same number of KV caches to attend to. However, CEPE is worse than original LLaMA in TTIT since it require the additional computation of KV cache projection in the inference time. Overall we achieve upto 6.78 $\\times$and $6 . 0 6 \\times$$\\times$acceleration in throughput much higher than CEPE in the long context scenario. "
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 15,
        "bbox": {
          "x1": 109,
          "y1": 353,
          "x2": 887,
          "y2": 536
        },
        "section_title": "A Additional Discussion "
      },
      "metadata": {
        "char_count": 1304
      }
    },
    {
      "id": "REFRAG_elem_000082",
      "type": "table",
      "content": {
        "html": "<table><tr><td></td><td>Acceleration/Save</td><td>Short s</td><td>Long s</td></tr><tr><td>KV cache memory</td><td>ks+ko/s+ko</td><td>1 ~ k×</td><td>k×</td></tr><tr><td>TTFT</td><td>k2(6ds+s2)/6dsk+s2</td><td>k×</td><td>k2×</td></tr><tr><td>TTIT</td><td>2dlbsk+nk+2dlbok/2dlbs+nk+2dlbok</td><td>1×</td><td>k×</td></tr><tr><td>Throughput</td><td>k*TTFT+k*TITT/TFT+kTTIT</td><td>1 ~ k×</td><td>k ~ k2×</td></tr></table>",
        "captions": [
          "Table 6 The acceleration in latency/save in memory of REFRAG compared to the original LLaMA model. "
        ],
        "description": ""
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 15,
        "bbox": {
          "x1": 215,
          "y1": 545,
          "x2": 782,
          "y2": 638
        },
        "section_title": "A Additional Discussion ",
        "image_path": "D:/Code_list/Code_Python/Agent_gragh_rag/files/file_store/md_store/minerU_work/REFRAG/images/710aac01ff4cd36ef00e65ccf583dc8e7e7bad0a851846b6003c0728c74779c5.jpg"
      },
      "metadata": {
        "table_type": "simple_table",
        "row_count": 4,
        "col_count": 4
      }
    },
    {
      "id": "REFRAG_elem_000083",
      "type": "title",
      "content": {
        "text": "A.1 Modeling REFRAG Selective Compression ",
        "level": 1
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 15,
        "bbox": {
          "x1": 109,
          "y1": 689,
          "x2": 503,
          "y2": 707
        }
      },
      "metadata": {
        "char_count": 42
      }
    },
    {
      "id": "REFRAG_elem_000084",
      "type": "paragraph",
      "content": {
        "text": "In this section, we introduce selective token compression, based on the hypothesis that different context segments contribute unequally to answer prediction. Less critical segments are compressed, while essential ones remain intact, as illustrated in figure 5. We employ RL to train a policy that optimally determines which segments to compress. "
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 15,
        "bbox": {
          "x1": 109,
          "y1": 713,
          "x2": 885,
          "y2": 773
        },
        "section_title": "A.1 Modeling REFRAG Selective Compression "
      },
      "metadata": {
        "char_count": 346
      }
    },
    {
      "id": "REFRAG_elem_000085",
      "type": "paragraph",
      "content": {
        "text": "To enable selective compression, we continue pretraining the encoder and decoder to process a combination of token and chunk embeddings. Given a context of $s$tokens $x _ { 1 } , \\ldots , x _ { s }$, chunked into $L$fixed-length chunks $C _ { 1 } , \\dots , C _ { L }$, we achieve a compression fraction of $1 - p$by randomly selecting $T ^ { \\prime } : = p L$chunks to remain uncompressed for the decoder. This pretraining allows the model to effectively handle mixed inputs at arbitrary positions, which is essential for the subsequent RL policy learning. "
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 15,
        "bbox": {
          "x1": 109,
          "y1": 782,
          "x2": 885,
          "y2": 858
        },
        "section_title": "A.1 Modeling REFRAG Selective Compression "
      },
      "metadata": {
        "char_count": 557
      }
    },
    {
      "id": "REFRAG_elem_000086",
      "type": "paragraph",
      "content": {
        "text": "We sequentially pick $T ^ { \\prime }$chunk indices $l = \\{ l _ { j } \\} _ { j = 1 } ^ { T ^ { \\prime } }$, where $l _ { t } \\in [ L ]$. The input arrangement is $E ( x , \\{ l _ { j } \\} _ { j = 1 } ^ { T ^ { \\prime } } ) =$$\\{ E _ { 1 } , \\dots , E _ { L } \\}$, with $E _ { i } = { \\bf e } _ { i } ^ { \\mathrm { c n k } }$if $i \\not \\in \\{ l _ { j } \\} _ { j = 1 } ^ { T ^ { \\prime } }$(compressed), and $E _ { i } = \\{ \\mathbf { e } _ { k * i } , \\dots , \\mathbf { e } _ { k * i + k - 1 } \\}$if $i \\in \\{ l _ { j } \\} _ { j = 1 } ^ { T ^ { \\prime } }$ (uncompressed). This arrangement is input to the decoder ${ \\mathcal { M } } _ { \\mathrm { d e c } }$to predict $x _ { s + 1 : s + o }$. The decoder’s autoregressive property is maintained, and compression can be applied at any position within the input, not just at the beginning. Within our selective compression framework, the objective is to choose $T ^ { \\prime }$chunks from $L$total chunks to maximize a specified reward. Formally, this can be expressed as the following combinatorial optimization problem: This problem is non-differentiable due to its discrete nature, and exact solutions are NP-hard. Consequently, prior work has proposed greedy approaches that incrementally construct solutions by modeling the task as a sequential decision-making problem (Dai et al., 2017; Bello et al., 2017). These studies show that such greedy formulations enable the use of RL to achieve near-optimal solutions and generalize well across diverse settings. Motivated by these findings, we adopt a sequential formulation for selective compression and employ RL to train an effective policy (see section 2). "
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 15,
        "bbox": {
          "x1": 109,
          "y1": 863,
          "x2": 883,
          "y2": 902
        },
        "section_title": "A.1 Modeling REFRAG Selective Compression "
      },
      "metadata": {
        "char_count": 1657
      }
    },
    {
      "id": "REFRAG_elem_000087",
      "type": "paragraph",
      "content": {
        "text": "We learn a policy network $\\pi \\theta$that takes chunk embeddings $\\{ { \\bf c } _ { i } \\} _ { i = 1 } ^ { L }$and sequentially selects $T ^ { \\prime }$chunk indices $l _ { 1 } , \\ldots , l _ { T ^ { \\prime } }$, where $l _ { t } \\in [ L ]$. At stage $t$, the policy samples from: where ${ \\mathrm { I I } } _ { j } ~ = ~ \\infty$iff $j \\in \\{ l _ { i } \\} _ { i = 1 } ^ { t - 1 }$and 0 otherwise4; $\\mathbf { s } ~ = ~ g _ { \\theta } ( \\{ \\mathbf { c } _ { i } \\} _ { i \\in [ L ] , i \\not \\in \\{ l _ { j } \\} _ { \\underline { { { i } } } = 1 } ^ { t - 1 } } )$is the output of a two-layer transformer network over chunk embeddings, producing logit ${ \\bf s } _ { i }$for each chunk. In practice, we reuse chunk embeddings $\\{ { \\bf c } _ { i } \\} _ { i = 1 } ^ { L }$as transformer input and do not recompute logits ${ \\bf s } _ { i }$after each selection, as state changes have minimal impact and this improves training speed. "
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 16,
        "bbox": {
          "x1": 109,
          "y1": 316,
          "x2": 887,
          "y2": 349
        },
        "section_title": "A.1 Modeling REFRAG Selective Compression "
      },
      "metadata": {
        "char_count": 928
      }
    },
    {
      "id": "REFRAG_elem_000088",
      "type": "paragraph",
      "content": {
        "text": "We use GRPO (Shao et al., 2024) style baseline to use grouped reward as baseline to reduce variance and to minimize contamination across different segment prediction task. Specifically, for each $x$we randomly select $G$number of length $T ^ { \\prime }$action sequences $\\{ l ^ { ( i ) } \\} _ { i = 1 } ^ { G }$. We have the following objective: where $\\epsilon$is the clipping hyperparameter in PPO (Schulman et al., 2017) for stable training, $\\theta$is the current policy and $\\theta _ { \\mathrm { o l d } }$is the policy fro the previous iteration, $A _ { t }$is the advantage function. We define our advantage function using the negative log-perplexity on the $o$tokens $\\mathbf { X } _ { s + 1 : s + o }$: We compute the advantage function following GRPO as: "
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 16,
        "bbox": {
          "x1": 107,
          "y1": 468,
          "x2": 887,
          "y2": 516
        },
        "section_title": "A.1 Modeling REFRAG Selective Compression "
      },
      "metadata": {
        "char_count": 765
      }
    },
    {
      "id": "REFRAG_elem_000089",
      "type": "title",
      "content": {
        "text": "B Additional Details on Experimental Settings ",
        "level": 1
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 16,
        "bbox": {
          "x1": 109,
          "y1": 768,
          "x2": 581,
          "y2": 787
        }
      },
      "metadata": {
        "char_count": 46
      }
    },
    {
      "id": "REFRAG_elem_000090",
      "type": "title",
      "content": {
        "text": "B.1 Additional Details on Baselines ",
        "level": 1
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 16,
        "bbox": {
          "x1": 109,
          "y1": 803,
          "x2": 413,
          "y2": 818
        }
      },
      "metadata": {
        "char_count": 36
      }
    },
    {
      "id": "REFRAG_elem_000091",
      "type": "paragraph",
      "content": {
        "text": "All baseline models are based on the LLaMA-2-7B model (Touvron et al., 2023), unless otherwise specified, to ensure a fair comparison since the previous methods are trained based on this model.5 We do provide results "
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 16,
        "bbox": {
          "x1": 107,
          "y1": 827,
          "x2": 885,
          "y2": 859
        },
        "section_title": "B.1 Additional Details on Baselines "
      },
      "metadata": {
        "char_count": 217
      }
    },
    {
      "id": "REFRAG_elem_000092",
      "type": "image",
      "content": {
        "captions": [
          "Figure 5 A demonstration of selective token compression. For all chunks, by default, we compress them to a single token, while for crucial chunks, we expand them. "
        ],
        "description": ""
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 17,
        "bbox": {
          "x1": 153,
          "y1": 85,
          "x2": 848,
          "y2": 290
        },
        "section_title": "B.1 Additional Details on Baselines ",
        "image_path": "D:/Code_list/Code_Python/Agent_gragh_rag/files/file_store/md_store/minerU_work/REFRAG/images/9d754df165861b17ffc4ea115d2e5aceafd4bf662fdb1f5a961b065d480f638a.jpg"
      }
    },
    {
      "id": "REFRAG_elem_000093",
      "type": "paragraph",
      "content": {
        "text": "on other encoder-decoder combinations in our ablation experiments (see section 4.1). Each data point contains $T = 4 0 9 6$tokens, where the first $s = 2 0 4 8$tokens are referred to as the context tokens, and the remaining $o = 2 0 4 8$tokens are the output tokens, such that $s + o = T$. We evaluate the perplexity on $x _ { s + 1 : s + o }$in this section. "
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 17,
        "bbox": {
          "x1": 109,
          "y1": 354,
          "x2": 883,
          "y2": 412
        },
        "section_title": "B.1 Additional Details on Baselines "
      },
      "metadata": {
        "char_count": 360
      }
    },
    {
      "id": "REFRAG_elem_000094",
      "type": "paragraph",
      "content": {
        "text": "LLaMA-No Context: The original pre-trained LLaMA model evaluated directly on $x _ { s + 1 : s + o }$with only $x _ { s + 1 : s + o }$as input. "
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 17,
        "bbox": {
          "x1": 109,
          "y1": 422,
          "x2": 883,
          "y2": 453
        },
        "section_title": "B.1 Additional Details on Baselines "
      },
      "metadata": {
        "char_count": 143
      }
    },
    {
      "id": "REFRAG_elem_000095",
      "type": "paragraph",
      "content": {
        "text": "LLaMA-Full Context: Similar to the LLaMA-No Context, we evaluate the perplexity on $x _ { s + 1 : s + o }$; however, we also input the whole sequence to the model, including the context tokens, i.e., $x _ { 1 : T }$. Therefore, the perplexity of this model is expected to be lower than LLaMA-No Context. The perplexity of this model serves as a reference, showing the upper bound of the performance of our model. "
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 17,
        "bbox": {
          "x1": 109,
          "y1": 460,
          "x2": 883,
          "y2": 521
        },
        "section_title": "B.1 Additional Details on Baselines "
      },
      "metadata": {
        "char_count": 413
      }
    },
    {
      "id": "REFRAG_elem_000096",
      "type": "paragraph",
      "content": {
        "text": "LLaMAK: Similar to the LLaMA-Full Context, we pass last $K$tokens $x _ { s _ { K } : s }$in addition to $x _ { s + 1 : s + o }$to compute perplexity in $x _ { s + 1 : s + o }$. The performance of LLaMA $\\boldsymbol { \\kappa }$falls between LLaMA-No Context and LLaMA-Full Context, making it a strong baseline for comparison with REFRAG when the number of context tokens is matched. "
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 17,
        "bbox": {
          "x1": 109,
          "y1": 527,
          "x2": 883,
          "y2": 588
        },
        "section_title": "B.1 Additional Details on Baselines "
      },
      "metadata": {
        "char_count": 382
      }
    },
    {
      "id": "REFRAG_elem_000097",
      "type": "paragraph",
      "content": {
        "text": "CEPE: A memory-efficient long-context model modified from the LLaMA model (Yen et al., 2024). The model architecture is similar to T5. We feed $x _ { 1 : s }$into their encoder model and evaluate the perplexity on the output tokens $x _ { s + 1 : s + o }$. CEPED refers to its instruction fine-tuned variant. "
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 17,
        "bbox": {
          "x1": 109,
          "y1": 595,
          "x2": 883,
          "y2": 642
        },
        "section_title": "B.1 Additional Details on Baselines "
      },
      "metadata": {
        "char_count": 309
      }
    },
    {
      "id": "REFRAG_elem_000098",
      "type": "paragraph",
      "content": {
        "text": "LLaMA-32K: A fine-tuned version of the original LLaMA-2 7B model that extends the context length from the original 4K to 32K. "
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 17,
        "bbox": {
          "x1": 109,
          "y1": 648,
          "x2": 882,
          "y2": 679
        },
        "section_title": "B.1 Additional Details on Baselines "
      },
      "metadata": {
        "char_count": 126
      }
    },
    {
      "id": "REFRAG_elem_000099",
      "type": "paragraph",
      "content": {
        "text": "REPLUG: A retrieval-augmented language modeling framework that uses different retrieved contexts to perform ensemble generation. We use REPLUG to refer to applying this framework on the LLaMA pre-trained model, REPLUGChat to refer to applying this framework on the LLaMA chat model (i.e., instruction fine-tuned), and REPLUGFT to refer to applying it on the LLaMA model fine-tuned on the downstream tasks (see section 5). "
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 17,
        "bbox": {
          "x1": 109,
          "y1": 686,
          "x2": 883,
          "y2": 762
        },
        "section_title": "B.1 Additional Details on Baselines "
      },
      "metadata": {
        "char_count": 422
      }
    },
    {
      "id": "REFRAG_elem_000100",
      "type": "paragraph",
      "content": {
        "text": "REFRAG: Our approach is illustrated in figure 1. We use RoBERTa-large (Liu et al., 2019) as the encoder, feeding $x _ { 1 : s }$tokens and evaluating the perplexity on the output tokens $x _ { s + 1 : s + o }$. We use REFRAG $k$to denote our model with compression rate of $k$. We use REFRAGRL to refer to the model with selective compression using our RL policy. "
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 17,
        "bbox": {
          "x1": 109,
          "y1": 768,
          "x2": 883,
          "y2": 830
        },
        "section_title": "B.1 Additional Details on Baselines "
      },
      "metadata": {
        "char_count": 364
      }
    },
    {
      "id": "REFRAG_elem_000101",
      "type": "title",
      "content": {
        "text": "B.2 Additional Details on Hyperparameters and Experimental Settings for CPT ",
        "level": 1
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 17,
        "bbox": {
          "x1": 109,
          "y1": 849,
          "x2": 766,
          "y2": 866
        }
      },
      "metadata": {
        "char_count": 76
      }
    },
    {
      "id": "REFRAG_elem_000102",
      "type": "paragraph",
      "content": {
        "text": "Hyperparameters. For reconstruction stage, we use a peak learning rate of $2 e - 4$since we only train the encoder model. For the next paragraph prediction we use a peak learning rate of $5 e - 5$since we train all the parameters in the model, including the decoder parameters. For all the instruction-tuning tasks, we use the peak learning rate of $2 e - 5$. We use a 4% linear warm-up stage for learning rate, AdamW optimizer (Loshchilov and Hutter, 2019), cosine learning rate scheduler and a batch size of 256 for all the experiments. For the projection layer, we use a 2-layer multi-layer perception (MLP) with an hidden size that is equivalent to the output size (i.e., 4096 for LLaMA-2-7B). For both tasks we train our model for 4 epochs on the dataset using the curriculum learning schedule (see figure 6). "
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 17,
        "bbox": {
          "x1": 109,
          "y1": 875,
          "x2": 883,
          "y2": 904
        },
        "section_title": "B.2 Additional Details on Hyperparameters and Experimental Settings for CPT "
      },
      "metadata": {
        "char_count": 815
      }
    },
    {
      "id": "REFRAG_elem_000103",
      "type": "paragraph",
      "content": {
        "text": "Computational Resources. We train all our models in Bfloat16 precision. We adopt Fully Sharded Data Parallel (FSDP) for all the experiments and train our model on 8 nodes with 8 H100 cards on each node. "
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 18,
        "bbox": {
          "x1": 109,
          "y1": 186,
          "x2": 885,
          "y2": 219
        },
        "section_title": "B.2 Additional Details on Hyperparameters and Experimental Settings for CPT "
      },
      "metadata": {
        "char_count": 203
      }
    },
    {
      "id": "REFRAG_elem_000104",
      "type": "paragraph",
      "content": {
        "text": "Evaluation metrics in $R A G$. Table 7 provides a summarization of the evaluation metrics we use for each dataset in RAG experiments. "
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 18,
        "bbox": {
          "x1": 109,
          "y1": 234,
          "x2": 885,
          "y2": 266
        },
        "section_title": "B.2 Additional Details on Hyperparameters and Experimental Settings for CPT "
      },
      "metadata": {
        "char_count": 134
      }
    },
    {
      "id": "REFRAG_elem_000105",
      "type": "paragraph",
      "content": {
        "text": "Experimental setting for fine-tuning model to take a combination of token and chunk embedding as input. We continue the model training from the continual pre-training checkpoint. To fine-tune the model, we set $p = 0 . 1$(i.e., compression 90% of the chunks) and randomly select $p L$chunks to keep their original token in the decoder. The input arrangement is the same as what we describe in section 2. "
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 18,
        "bbox": {
          "x1": 109,
          "y1": 282,
          "x2": 887,
          "y2": 343
        },
        "section_title": "B.2 Additional Details on Hyperparameters and Experimental Settings for CPT "
      },
      "metadata": {
        "char_count": 404
      }
    },
    {
      "id": "REFRAG_elem_000106",
      "type": "table",
      "content": {
        "html": "<table><tr><td>Dataset</td><td>Metric</td></tr><tr><td>OpenAssistant Conversations</td><td>F1</td></tr><tr><td>CommonsenseQA</td><td>Accuracy</td></tr><tr><td>MathQA</td><td>Accuracy</td></tr><tr><td>Web Questions</td><td>Exact Match</td></tr><tr><td>WikiQA</td><td>F1</td></tr><tr><td>Yahoo! Answers QA</td><td>F1</td></tr><tr><td>FreebaseQA</td><td>Exact Match</td></tr><tr><td>MS MARCO</td><td>F1</td></tr><tr><td>PubMedQA</td><td>Exact Match</td></tr><tr><td>QuaRel</td><td>Accuracy</td></tr><tr><td>GSM8K</td><td>Exact Match</td></tr><tr><td>StrategyQA</td><td>Exact Match</td></tr><tr><td>MMLU</td><td>Accuracy</td></tr><tr><td>BoolQ</td><td>Exact Match</td></tr><tr><td>SIQA</td><td>Accuracy</td></tr><tr><td>PIQA</td><td>Accuracy</td></tr><tr><td>HellaSwag</td><td>Accuracy</td></tr><tr><td>Winogrande</td><td>Accuracy</td></tr><tr><td>TriviaQA</td><td>Exact Match</td></tr><tr><td>FEVER</td><td>Exact Match</td></tr><tr><td>NQ</td><td>Exact Match</td></tr></table>",
        "captions": [
          "Table 7 Metrics used for each dataset in RAG experiments in table 3 "
        ],
        "description": ""
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 18,
        "bbox": {
          "x1": 326,
          "y1": 353,
          "x2": 671,
          "y2": 690
        },
        "section_title": "B.2 Additional Details on Hyperparameters and Experimental Settings for CPT ",
        "image_path": "D:/Code_list/Code_Python/Agent_gragh_rag/files/file_store/md_store/minerU_work/REFRAG/images/db98bf1fcc4cb4a71e532cfabbcaecd6047563dd37bc1dca30f6c517cc2aeb48.jpg"
      },
      "metadata": {
        "table_type": "simple_table",
        "row_count": 21,
        "col_count": 2
      }
    },
    {
      "id": "REFRAG_elem_000107",
      "type": "title",
      "content": {
        "text": "B.3 Curriculum learning data mixture ",
        "level": 1
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 18,
        "bbox": {
          "x1": 109,
          "y1": 739,
          "x2": 429,
          "y2": 757
        }
      },
      "metadata": {
        "char_count": 37
      }
    },
    {
      "id": "REFRAG_elem_000108",
      "type": "paragraph",
      "content": {
        "text": "Table 8 presents the number of data points used at each training stage of our model. We employ a geometric sequence for each type of data point, based on the intuition that training should begin with a greater proportion of easier examples and gradually introduce more challenging ones as training progresses. The right-most column indicates the total number of data points for each type. We allocate more data points to longer-context examples to encourage the model to focus on learning more difficult tasks. "
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 18,
        "bbox": {
          "x1": 109,
          "y1": 765,
          "x2": 887,
          "y2": 842
        },
        "section_title": "B.3 Curriculum learning data mixture "
      },
      "metadata": {
        "char_count": 511
      }
    },
    {
      "id": "REFRAG_elem_000109",
      "type": "title",
      "content": {
        "text": "B.4 Detailed Calculation of Acceleration in Latency and Throughput of Our Model ",
        "level": 1
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 18,
        "bbox": {
          "x1": 109,
          "y1": 859,
          "x2": 790,
          "y2": 876
        }
      },
      "metadata": {
        "char_count": 80
      }
    },
    {
      "id": "REFRAG_elem_000110",
      "type": "paragraph",
      "content": {
        "text": "In this section, we provide a detailed analysis of the TTFT and generation latency for the LLaMA-2 model. We denote the following parameters: $s$as the context length, $o$as the output length, $b$as the batch size, $d$as "
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 18,
        "bbox": {
          "x1": 109,
          "y1": 883,
          "x2": 887,
          "y2": 914
        },
        "section_title": "B.4 Detailed Calculation of Acceleration in Latency and Throughput of Our Model "
      },
      "metadata": {
        "char_count": 221
      }
    },
    {
      "id": "REFRAG_elem_000111",
      "type": "image",
      "content": {
        "captions": [
          "Figure 6 The data mixture in curriculum learning during the training. "
        ],
        "description": ""
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 19,
        "bbox": {
          "x1": 272,
          "y1": 83,
          "x2": 718,
          "y2": 272
        },
        "section_title": "B.4 Detailed Calculation of Acceleration in Latency and Throughput of Our Model ",
        "image_path": "D:/Code_list/Code_Python/Agent_gragh_rag/files/file_store/md_store/minerU_work/REFRAG/images/f2b5ca26598e74830a89c374a131459ec7e36267c1b23d0929aab674e9797590.jpg"
      }
    },
    {
      "id": "REFRAG_elem_000112",
      "type": "table",
      "content": {
        "html": "<table><tr><td>Factor</td><td>Stage 1</td><td>Stage 2</td><td>Stage 3</td><td>Stage 4</td><td>Stage 5</td><td>Stage 6</td><td>Stage 7</td><td>Stage 8</td><td>Stage 9</td><td>Summation</td></tr><tr><td>1 × 8</td><td>1333</td><td>445</td><td>148</td><td>49</td><td>16</td><td>6</td><td>2</td><td>1</td><td>0</td><td>2000</td></tr><tr><td>2 × 8</td><td>333</td><td>298</td><td>267</td><td>238</td><td>213</td><td>191</td><td>171</td><td>153</td><td>137</td><td>2000</td></tr><tr><td>4 × 8</td><td>83</td><td>102</td><td>126</td><td>156</td><td>193</td><td>238</td><td>293</td><td>362</td><td>447</td><td>2000</td></tr><tr><td>8 × 8</td><td>20</td><td>35</td><td>61</td><td>106</td><td>185</td><td>324</td><td>565</td><td>985</td><td>1719</td><td>4000</td></tr><tr><td>16 × 8</td><td>5</td><td>11</td><td>23</td><td>48</td><td>103</td><td>220</td><td>468</td><td>997</td><td>2125</td><td>4000</td></tr><tr><td>32 × 8</td><td>1</td><td>3</td><td>7</td><td>19</td><td>50</td><td>133</td><td>353</td><td>939</td><td>2496</td><td>4000</td></tr><tr><td>64 × 8</td><td>1</td><td>3</td><td>9</td><td>25</td><td>73</td><td>212</td><td>618</td><td>1802</td><td>5259</td><td>8000</td></tr><tr><td>128 × 8</td><td>1</td><td>3</td><td>9</td><td>25</td><td>73</td><td>212</td><td>618</td><td>1802</td><td>5259</td><td>8000</td></tr><tr><td>256 × 8</td><td>1</td><td>3</td><td>9</td><td>25</td><td>73</td><td>212</td><td>618</td><td>1802</td><td>5259</td><td>8000</td></tr></table>",
        "captions": [
          "Table 8 The geometry curriculum learning scheduling. The whole training is split into 9 stages. In each stage, we have a combination of different data (e.g., 1X8 means reconstructing 8 tokens, 2X8 means reconstructing 16 tokens). For each type of data, the number of samples in each stage is determined by a geometric sequence which sums up to the total number of samples in the last column. As training proceeds, the data mixture has more and more longer sequences. "
        ],
        "description": ""
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 19,
        "bbox": {
          "x1": 117,
          "y1": 316,
          "x2": 879,
          "y2": 463
        },
        "section_title": "B.4 Detailed Calculation of Acceleration in Latency and Throughput of Our Model ",
        "image_path": "D:/Code_list/Code_Python/Agent_gragh_rag/files/file_store/md_store/minerU_work/REFRAG/images/d6d27e1909feb2ad596e7c07a083a68ab832cf0221a7edb355fe1eec288885cd.jpg"
      },
      "metadata": {
        "table_type": "simple_table",
        "row_count": 9,
        "col_count": 11
      }
    },
    {
      "id": "REFRAG_elem_000113",
      "type": "paragraph",
      "content": {
        "text": "the dimensionality of the hidden states, $l$as the number of layers in the decoder, and $n$as the number of model parameters. The flop rate of the GPU is $f$, and the high bandwidth memory of the GPU is $m$. The model is loaded with bfloat16 precision. We focus our analysis on LLaMA-2-7B model. The results should be generalizable to other models. "
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 19,
        "bbox": {
          "x1": 109,
          "y1": 569,
          "x2": 883,
          "y2": 630
        },
        "section_title": "B.4 Detailed Calculation of Acceleration in Latency and Throughput of Our Model "
      },
      "metadata": {
        "char_count": 349
      }
    },
    {
      "id": "REFRAG_elem_000114",
      "type": "paragraph",
      "content": {
        "text": "TTFT: Computationally Bounded Analysis Existing work (Liu et al., 2025) has shown that the TTFT latency is primarily limited by computation. The primary computations in each layer of LLaMA-2 involve attention calculations and feedforward layers. We follow the analysis in (Liu et al., 2025) to calculate the TTFT. Note that each operation involves both a multiplication and an addition, hence we multiply the flop count by 2. "
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 19,
        "bbox": {
          "x1": 109,
          "y1": 647,
          "x2": 883,
          "y2": 724
        },
        "section_title": "B.4 Detailed Calculation of Acceleration in Latency and Throughput of Our Model "
      },
      "metadata": {
        "char_count": 426
      }
    },
    {
      "id": "REFRAG_elem_000115",
      "type": "title",
      "content": {
        "text": "• Attention Calculation: ",
        "level": 1
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 19,
        "bbox": {
          "x1": 135,
          "y1": 732,
          "x2": 307,
          "y2": 744
        }
      },
      "metadata": {
        "char_count": 25
      }
    },
    {
      "id": "REFRAG_elem_000116",
      "type": "list",
      "content": {
        "text": "– QKV Projection: Transforms input from [ b , s , d ]to [ d , 3 d ], requiring 6 b s d ^ { 2 }flops. – Attention Score Calculation: Q K ^ { T }operation from [ b , h , s , d / h ] \\times [ b , h , d / h , s ], requiring 2 b d s ^ { 2 }flops. – Attention Output Calculation: Weighted average of the value hidden state, [ b , h , s , s ] \\times [ b , h , s , d / h ]requiring 2 b d s ^ { 2 }flops. – Output Projection: [ b , s , d ] \\times [ d , d ], requiring 2 b s d ^ { 2 }flops. "
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 19,
        "bbox": {
          "x1": 166,
          "y1": 753,
          "x2": 887,
          "y2": 852
        },
        "section_title": "• Attention Calculation: "
      },
      "metadata": {
        "char_count": 481
      }
    },
    {
      "id": "REFRAG_elem_000117",
      "type": "paragraph",
      "content": {
        "text": "The total flops for attention is $8 b s d ^ { 2 } + 4 b d s ^ { 2 }$. "
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 19,
        "bbox": {
          "x1": 148,
          "y1": 859,
          "x2": 478,
          "y2": 873
        },
        "section_title": "• Attention Calculation: "
      },
      "metadata": {
        "char_count": 70
      }
    },
    {
      "id": "REFRAG_elem_000118",
      "type": "paragraph",
      "content": {
        "text": "• Feedforward Layer: In LLaMA-2-7B, the MLP layer first projects to $2 . 6 8 7 5 d$with a gated function and then back to $d$. Each projection requires $5 . 3 7 5 b s d ^ { 2 }$flops. With three such operations, the total is $1 6 . 1 2 5 b s d ^ { 2 }$. "
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 19,
        "bbox": {
          "x1": 135,
          "y1": 882,
          "x2": 883,
          "y2": 912
        },
        "section_title": "• Attention Calculation: "
      },
      "metadata": {
        "char_count": 254
      }
    },
    {
      "id": "REFRAG_elem_000119",
      "type": "paragraph",
      "content": {
        "text": "• Total Computation per Layer: Summing the above, each layer requires approximately $2 4 b s d ^ { 2 } + 4 b d s ^ { 2 }$flops. "
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 20,
        "bbox": {
          "x1": 135,
          "y1": 103,
          "x2": 887,
          "y2": 119
        },
        "section_title": "• Attention Calculation: "
      },
      "metadata": {
        "char_count": 128
      }
    },
    {
      "id": "REFRAG_elem_000120",
      "type": "paragraph",
      "content": {
        "text": "For a sequence length $s$, number of layers $\\it l$, and batch size $b$, the total computation for pre-fill is $( 2 4 d ^ { 2 } + 4 d s ) l b s$. Given the flop rate $f$, the latency for pre-fill is dominated by computation, yielding a final latency of $\\frac { ( 2 4 d ^ { 2 } + 4 d s ) l b s } { f }$ Generation analysis: Memory bounded Analysis For generation latency, existing work have shown that the generation process is memory bounded (Shi et al., 2025) which requires transferring KV cache and model parameter to high-bandwidth memory, we analyse the data transfer latency as follows: "
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 20,
        "bbox": {
          "x1": 109,
          "y1": 125,
          "x2": 887,
          "y2": 178
        },
        "section_title": "• Attention Calculation: "
      },
      "metadata": {
        "char_count": 594
      }
    },
    {
      "id": "REFRAG_elem_000121",
      "type": "title",
      "content": {
        "text": "• Memory Latency: ",
        "level": 1
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 20,
        "bbox": {
          "x1": 135,
          "y1": 244,
          "x2": 276,
          "y2": 258
        }
      },
      "metadata": {
        "char_count": 18
      }
    },
    {
      "id": "REFRAG_elem_000122",
      "type": "list",
      "content": {
        "text": "– KV Cache Data: Requires 4 d l b ( s + o )bytes (bfloat16 uses 2 bytes per number, and there are separate key/value copies). – Model Parameters: Require 2 nbytes. "
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 20,
        "bbox": {
          "x1": 166,
          "y1": 266,
          "x2": 883,
          "y2": 320
        },
        "section_title": "• Memory Latency: "
      },
      "metadata": {
        "char_count": 164
      }
    },
    {
      "id": "REFRAG_elem_000123",
      "type": "paragraph",
      "content": {
        "text": "The data transfer latency to high-bandwidth memory is $\\frac { 2 n + 4 d l b ( s + o ) } { m }$ Throughput Calculation The throughput, defined as the number of tokens generated per unit time, is given by: where DL is the data latency. "
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 20,
        "bbox": {
          "x1": 150,
          "y1": 325,
          "x2": 648,
          "y2": 348
        },
        "section_title": "• Memory Latency: "
      },
      "metadata": {
        "char_count": 235
      }
    },
    {
      "id": "REFRAG_elem_000124",
      "type": "table",
      "content": {
        "html": "<table><tr><td></td><td>Before</td><td>After</td></tr><tr><td>KV cache memory</td><td>4ddb(s + o)</td><td>4ddb( s/k + o)</td></tr><tr><td>TTFT</td><td>(24d2+4ds)lbsf</td><td>(24d2+4ds/k)lbsk/f</td></tr><tr><td>TTIT</td><td>2n+4ddb(s+o)</td><td>2n+4ddb( s/k + o)</td></tr><tr><td rowspan=\"2\">Throughput</td><td>mbo</td><td>mbo</td></tr><tr><td>TTFTbefore+TTITbefore</td><td>TTFTafter+TTITafter</td></tr></table>",
        "captions": [
          "Table 9 Comparison of KV cache memory usage, TTFT, generation latency and throughput between the original LLaMA model and our model. "
        ],
        "description": ""
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 20,
        "bbox": {
          "x1": 277,
          "y1": 450,
          "x2": 718,
          "y2": 545
        },
        "section_title": "• Memory Latency: ",
        "image_path": "D:/Code_list/Code_Python/Agent_gragh_rag/files/file_store/md_store/minerU_work/REFRAG/images/458d563544dcfd7191cc6dbe924b4e1a922c4bd72ee52ac68da978148f502ca9.jpg"
      },
      "metadata": {
        "table_type": "complex_table",
        "row_count": 5,
        "col_count": 2
      }
    },
    {
      "id": "REFRAG_elem_000125",
      "type": "title",
      "content": {
        "text": "B.5 Additional details on empirical measurement of latency and memory improvement in figure 2, figure 9 and figure 8 ",
        "level": 1
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 20,
        "bbox": {
          "x1": 109,
          "y1": 612,
          "x2": 888,
          "y2": 646
        }
      },
      "metadata": {
        "char_count": 117
      }
    },
    {
      "id": "REFRAG_elem_000126",
      "type": "paragraph",
      "content": {
        "text": "We measure the latency and memory usage in a controlled environment which aims to reduce other environmental factors that could make certain method advantageous. "
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 20,
        "bbox": {
          "x1": 107,
          "y1": 654,
          "x2": 887,
          "y2": 685
        },
        "section_title": "B.5 Additional details on empirical measurement of latency and memory improvement in figure 2, figure 9 and figure 8 "
      },
      "metadata": {
        "char_count": 162
      }
    },
    {
      "id": "REFRAG_elem_000127",
      "type": "paragraph",
      "content": {
        "text": "To this end, our implementation uses the same modelling file which means different baselines share the same hyper-parameter and acceleration (e.g., flash-attention). Therefore, we restrict the factors that affect the resource usage only among the model designs. We use the batch size of 1 and use a single A100 card to measure the system performance. "
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 20,
        "bbox": {
          "x1": 107,
          "y1": 691,
          "x2": 887,
          "y2": 752
        },
        "section_title": "B.5 Additional details on empirical measurement of latency and memory improvement in figure 2, figure 9 and figure 8 "
      },
      "metadata": {
        "char_count": 351
      }
    },
    {
      "id": "REFRAG_elem_000128",
      "type": "title",
      "content": {
        "text": "C Additional Experimental Results ",
        "level": 1
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 20,
        "bbox": {
          "x1": 109,
          "y1": 775,
          "x2": 468,
          "y2": 794
        }
      },
      "metadata": {
        "char_count": 34
      }
    },
    {
      "id": "REFRAG_elem_000129",
      "type": "paragraph",
      "content": {
        "text": "Sparse attention across different retrieved passages. We retrieve 200 passages using the query “how bruce lee died” from our retrieval corpus. We choose 5 passages that are different from each other (table 10) to simulate the de-duplication process in real RAG applications. We concatenate these 5 passages and feed it to LLaMA-2-7B-Chat model to see the attention values between different tokens. Figure 7 shows that the attention values for tokens within each passages are significantly larger than attention values for tokens in different passages which suggests redundancy in the current attention computation for RAG applications. "
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 20,
        "bbox": {
          "x1": 107,
          "y1": 806,
          "x2": 885,
          "y2": 898
        },
        "section_title": "C Additional Experimental Results "
      },
      "metadata": {
        "char_count": 636
      }
    },
    {
      "id": "REFRAG_elem_000130",
      "type": "image",
      "content": {
        "captions": [
          "Figure 7 Attention value visualization for different retrieved passages for different layers for LLaMA-2-7B-Chat model. The diagonal values are the averaged attention value for tokens within each passage while the off-diagonal values are the averaged attention value between tokens from different passages. The detail of retrieved passages is in table 10. "
        ],
        "description": ""
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 21,
        "bbox": {
          "x1": 305,
          "y1": 82,
          "x2": 689,
          "y2": 664
        },
        "section_title": "C Additional Experimental Results ",
        "image_path": "D:/Code_list/Code_Python/Agent_gragh_rag/files/file_store/md_store/minerU_work/REFRAG/images/a412f6ecefb6abd94d16c8b4d0432842743407c9d9359c0fd4be8bd3b3f03744.jpg"
      }
    },
    {
      "id": "REFRAG_elem_000131",
      "type": "paragraph",
      "content": {
        "text": "Additional results in latency measurement. Figure 9 and figure 8 shows the latency comparison of different models when using $k = 8$and $k = 3 2$compression rate for REFRAGrespectively. "
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 21,
        "bbox": {
          "x1": 109,
          "y1": 744,
          "x2": 887,
          "y2": 777
        },
        "section_title": "C Additional Experimental Results "
      },
      "metadata": {
        "char_count": 186
      }
    },
    {
      "id": "REFRAG_elem_000132",
      "type": "paragraph",
      "content": {
        "text": "Ablation study result for curriculum learning. Table 11 shows the necessity of curriculum learning to the success of reconstruction task. "
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 21,
        "bbox": {
          "x1": 109,
          "y1": 792,
          "x2": 887,
          "y2": 825
        },
        "section_title": "C Additional Experimental Results "
      },
      "metadata": {
        "char_count": 138
      }
    },
    {
      "id": "REFRAG_elem_000133",
      "type": "paragraph",
      "content": {
        "text": "Ablation study result for reconstruction task. Table 12 shows the performance comparison in CPT with and without continuing from reconstruction task. "
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 21,
        "bbox": {
          "x1": 109,
          "y1": 840,
          "x2": 887,
          "y2": 872
        },
        "section_title": "C Additional Experimental Results "
      },
      "metadata": {
        "char_count": 150
      }
    },
    {
      "id": "REFRAG_elem_000134",
      "type": "image",
      "content": {
        "captions": [
          "Figure 8 Empirical verification of inference acceleration of REFRAG with ",
          "k = 3 2"
        ],
        "description": ""
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 22,
        "bbox": {
          "x1": 261,
          "y1": 217,
          "x2": 720,
          "y2": 234
        },
        "section_title": "C Additional Experimental Results ",
        "image_path": "D:/Code_list/Code_Python/Agent_gragh_rag/files/file_store/md_store/minerU_work/REFRAG/images/0f53e8d396fa2966dcbb557a435b06caf9245a7b52fc368ac81dcd50f7a6e2df.jpg"
      }
    },
    {
      "id": "REFRAG_elem_000135",
      "type": "image",
      "content": {
        "captions": [
          "Figure 9 Empirical verification of inference acceleration of REFRAG with ",
          "k = 8"
        ],
        "description": ""
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 22,
        "bbox": {
          "x1": 261,
          "y1": 417,
          "x2": 720,
          "y2": 435
        },
        "section_title": "C Additional Experimental Results ",
        "image_path": "D:/Code_list/Code_Python/Agent_gragh_rag/files/file_store/md_store/minerU_work/REFRAG/images/1cda28df02caa26b56246a661ee50881ef66a1f2f6fb0bfbecbc40f159cf25ce.jpg"
      }
    },
    {
      "id": "REFRAG_elem_000136",
      "type": "paragraph",
      "content": {
        "text": "Ablation study result for the advantage of RL. Table 13 shows the advantage of using our selective compression policy via RL compared to using a lower compression rate. "
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 22,
        "bbox": {
          "x1": 109,
          "y1": 493,
          "x2": 883,
          "y2": 523
        },
        "section_title": "C Additional Experimental Results "
      },
      "metadata": {
        "char_count": 169
      }
    },
    {
      "id": "REFRAG_elem_000137",
      "type": "paragraph",
      "content": {
        "text": "Ablation study result of different compression rates. Figure 10 shows the loss trajectory for different compression rate of REFRAG. "
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 22,
        "bbox": {
          "x1": 109,
          "y1": 541,
          "x2": 883,
          "y2": 570
        },
        "section_title": "C Additional Experimental Results "
      },
      "metadata": {
        "char_count": 132
      }
    },
    {
      "id": "REFRAG_elem_000138",
      "type": "paragraph",
      "content": {
        "text": "Ablation study result of different combination of encoder and decoder models. Figure 11 shows the performance of CPT with different combination of encoder and decoder models. Table 14 shows the performance on LLaMA-3.1-8B and LLaMA-3.2-3B model. "
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 22,
        "bbox": {
          "x1": 109,
          "y1": 588,
          "x2": 883,
          "y2": 633
        },
        "section_title": "C Additional Experimental Results "
      },
      "metadata": {
        "char_count": 246
      }
    },
    {
      "id": "REFRAG_elem_000139",
      "type": "paragraph",
      "content": {
        "text": "Additional results in RAG. Table 16 shows the performance of different baselines under the same number of context. The performance of our model is similar to other methods, in other words no model significantly "
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 22,
        "bbox": {
          "x1": 109,
          "y1": 651,
          "x2": 883,
          "y2": 683
        },
        "section_title": "C Additional Experimental Results "
      },
      "metadata": {
        "char_count": 211
      }
    },
    {
      "id": "REFRAG_elem_000140",
      "type": "image",
      "content": {
        "captions": [
          "Figure 10 Training trajectory for our model with different compression rate. "
        ],
        "description": ""
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 22,
        "bbox": {
          "x1": 310,
          "y1": 708,
          "x2": 684,
          "y2": 878
        },
        "section_title": "C Additional Experimental Results ",
        "image_path": "D:/Code_list/Code_Python/Agent_gragh_rag/files/file_store/md_store/minerU_work/REFRAG/images/f2c1903d3fd0ed27b734c8bf95233ad823d5591afc8391225912f3eda182440e.jpg"
      }
    },
    {
      "id": "REFRAG_elem_000141",
      "type": "image",
      "content": {
        "captions": [
          "Figure 11 Training trajectory for different encoder and decoder combinations. On the left, we have two different decoder the Roberta-Base encoder. On the right we have two different encoder for LLaMA-2-7B decoder model. ",
          "Figure 12 Training trajectory for different encoder paired with LLaMA-2-13B decoder. "
        ],
        "description": ""
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 23,
        "bbox": {
          "x1": 330,
          "y1": 295,
          "x2": 666,
          "y2": 448
        },
        "section_title": "C Additional Experimental Results ",
        "image_path": "D:/Code_list/Code_Python/Agent_gragh_rag/files/file_store/md_store/minerU_work/REFRAG/images/4351376ba5119795c140d5dc055f006af8bc78005cf3c12d6d7f0bfac7030b2b.jpg"
      }
    },
    {
      "id": "REFRAG_elem_000142",
      "type": "paragraph",
      "content": {
        "text": "outperforms others. Table 15 shows the performance of REFRAG under different number of context for strong retriever setting. "
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 23,
        "bbox": {
          "x1": 109,
          "y1": 503,
          "x2": 883,
          "y2": 532
        },
        "section_title": "C Additional Experimental Results "
      },
      "metadata": {
        "char_count": 125
      }
    },
    {
      "id": "REFRAG_elem_000143",
      "type": "paragraph",
      "content": {
        "text": "Demonstration of generated summary for Arxiv and Pubmed articles. Table 20 and table 19 shows the ground true abstract for different articles and the generated summary from REFRAG. These results complement the perplexity results we have shown in CPT and accuracy/F1 performance we have shown in RAG and other applications. "
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 23,
        "bbox": {
          "x1": 109,
          "y1": 551,
          "x2": 883,
          "y2": 612
        },
        "section_title": "C Additional Experimental Results "
      },
      "metadata": {
        "char_count": 323
      }
    },
    {
      "id": "REFRAG_elem_000144",
      "type": "title",
      "content": {
        "text": "C.1 Additional Contextual Application - Summarization Task ",
        "level": 1
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 23,
        "bbox": {
          "x1": 109,
          "y1": 631,
          "x2": 614,
          "y2": 648
        }
      },
      "metadata": {
        "char_count": 59
      }
    },
    {
      "id": "REFRAG_elem_000145",
      "type": "paragraph",
      "content": {
        "text": "We fine-tune our model on the long document summarization dataset (Cohan et al., 2018). This dataset contains long scientific articles from Arxiv and Pubmed, and the task is to generate the abstract given the entire article. This application is challenging due to the long-context nature of the task. We fine-tune the REFRAG and LLaMA models on these two datasets and report the performance on the validation set. The summarization task provides an ideal condition to inspect whether it is beneficial to bring more information with compressed representation or less information without compression, since correct summarization requires complete information from the whole document. "
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 23,
        "bbox": {
          "x1": 109,
          "y1": 655,
          "x2": 883,
          "y2": 760
        },
        "section_title": "C.1 Additional Contextual Application - Summarization Task "
      },
      "metadata": {
        "char_count": 682
      }
    },
    {
      "id": "REFRAG_elem_000146",
      "type": "paragraph",
      "content": {
        "text": "Result analysis. Table 21 shows the performance of different baselines under the same number of tokens in the decoder. REPLUGFT means that we adopt the REPLUG framework using LLaMAFT, and REPLUGChat means that we adopt the LLaMA-2-7B-Chat model for REPLUG. We did not report some of our methods for certain decoder token counts since there were not enough input tokens for those compression rates. Our model achieves the best performance under the same number of decoder tokens (i.e., same latency). Additionally, REFRAG16 performs better than REFRAG $\\aleph$at a decoder token count of 128, since the former model is able to incorporate more information from the document with a higher compression rate. "
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 23,
        "bbox": {
          "x1": 109,
          "y1": 768,
          "x2": 883,
          "y2": 875
        },
        "section_title": "C.1 Additional Contextual Application - Summarization Task "
      },
      "metadata": {
        "char_count": 705
      }
    },
    {
      "id": "REFRAG_elem_000147",
      "type": "table",
      "content": {
        "html": "<table><tr><td></td><td>Content</td></tr><tr><td>P0</td><td>&quot;Water is necessary to survive, but as we all know, sometimes too much of a good thing (even water) can be harmful. In 2022, a group of kidney specialists from Madrid, Spain, revisited the death of Kung Fu legend Bruce Lee and concluded that water intoxication was the most likely cause of his untimely death. Bruce Lee, the martial arts legend and iconic figure in the history of cinema, died on July 20, 1973, at the young age of 32. The official cause of death at the time was reported as a probable drug reaction and classified as &quot;death by misadventure.&quot; Hours before his death, Lee complained of a headache while visiting a fellow actress Betty Ting Pei at her apartment. She gave him one of her own prescription painkillers (one that contained aspirin and meprobamate), and he laid down to take a nap. He never woke up and was unable to be resuscitated even after being transferred to a Hong Kong hospital. In the years since Lee&#x27;s death, many theories have been put forward as to the true cause of his passing. These theories include murder by gangsters or a jilted lover, a family curse, epilepsy, heatstroke, and possibly</td></tr><tr><td>P1</td><td>Bruce Lee May Have Died From Drinking Too Much Water, Claims Study The &#x27;Enter The Dragon&#x27; actor, who helped bring martial arts into popular culture, died in July 1973 at the age of 32. American martial arts legend and actor Bruce Lee might have died from drinking too much water, scientists have claimed in a new study. The &#x27;Enter The Dragon&#x27; actor, who helped bring martial arts into popular culture, died in July 1973 at the age of 32 from cerebral oedema, a swelling of the brain. At the time, doctors believed the brain swelling was due to a painkiller. The oedema, according to a group of researchers, was brought on by hyponatraemia. In their study, which was published in the Clinical Kidney Journal, the researchers proposed that Bruce Lee died because his kidneys were unable to eliminate extra water. The findings are very different from old theories about how died, such as those regarding gangster assassination, jealous lover poisoning, curses, and heatstroke. According to scientists, the actor may have died from hyponatraemia, which develops when the body&#x27;s sodium levels get diluted as a result of consuming too much water. The cells in the body, particularly those in the brain,</td></tr><tr><td>P2</td><td>circumstances, you&#x27;re bound to get some truly insane conspiracy theories, and there are plenty about Bruce Lee. The crazy Bruce Lee murder theories Producer Raymond Chow made a big mistake after Bruce Lee&#x27;s death. Hoping to protect Lee&#x27;s image, Chow&#x27;s production company claimed the actor died at home with his wife, Linda. But once the press found out the truth, the tabloids got going. In fact, a lot of people pointed the finger at Betty Ting Pei, claiming she was responsible for Lee&#x27;s death and that perhaps she&#x27;d even poisoned him. Unfortunately, that wasn&#x27;t the only rumor involving murder. One of the most popular theories says other martial artists were angry at Lee for teaching their secrets to Westerners, so they decided to bump him off. Some say ninjas were responsible, and others claim Lee was killed with the &quot;Dim Mak,&quot; a mythical martial arts move that supposedly kills a victim with one fateful blow. Others believe he was killed after refusing to pay protection money to the Triads, while others claim the Mafia did the deed because Lee wouldn&#x27;t let them control his career. The more mystical conspiracy theorists even say there&#x27;s a family curse that took the life</td></tr><tr><td>P3</td><td>Bruce Lee complained of a headache, was given an Equagesic — a painkiller that contains both aspirin and the tranquilizer meprobamate — and went down for a nap. He never woke up. His death was said to be an allergic reaction to the tranquilizer resulting in a cerebral edema (he had suffered a previous edema months before), though others claim his death was due to a negative reaction to cannabis, which Lee consumed regularly to reduce stress. Because he was so young, news of his death invited wild media speculation, from murder to a family curse. 5. Brandon Lee Sadly, Bruce Lee&#x27;s son Brandon also died young, at age 28, and also under strange circumstances. While filming the horror film The Crow, Lee was accidentally killed by a prop gun that, due to a malfunction in a previous scene, was accidentally loaded with a dummy bullet and a live primer. When the gun was fired, the bullet was ejected with virtually the same force as if loaded with a live round. Lee was hit in the abdomen and died in surgery later that day, on March 31, 1993. Like his father, Brandon&#x27;s abrupt death fed rumors. Conspiracy theorists believe Illuminati</td></tr><tr><td>P4</td><td>Bruce Lee moved to a house in Hong Kong&#x27;s Kowloon Tong district, it was said that the building suffered from bad feng shui. According to Lee biographer Bruce Thomas, the house&#x27;s two previous owners had financial issues, and the building &quot;faced the wrong way,&quot; and had disturbed natural winds. To fix this problem, a feng shui adviser ordered a mirror to be put on the roof. This was supposed to deflect the bad energy, but the mirror was knocked off during a typhoon. Ominously, Lee died just two days after the charm was blown away. While some of Lee&#x27;s neighbors apparently linked the two events at the time, the problem with this theory is that feng shui is nothing but a superstition. There&#x27;s no scientific evidence for any of its tenets, including qi. At most, feng shui could be regarded as a kind of art. Lee&#x27;s death after the loss of his mirror is a simple coincidence. Moreover, Lee died in Betty Ting&#x27;s apartment, not in his own house. 2. Murder The abruptness of Bruce Lee&#x27;s death, combined with his extraordinary fitness, made some fans wonder whether something more sinister was at work. People who believe that Lee was murdered</td></tr></table>",
        "captions": [
          "Table 10 The 5 retrieved passages for the query “how bruce lee died”. "
        ],
        "description": ""
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 24,
        "bbox": {
          "x1": 171,
          "y1": 122,
          "x2": 823,
          "y2": 893
        },
        "section_title": "C.1 Additional Contextual Application - Summarization Task ",
        "image_path": "D:/Code_list/Code_Python/Agent_gragh_rag/files/file_store/md_store/minerU_work/REFRAG/images/7eb4cfa1b0b73040a6b6d392345511f53e877121da55ada55c73ab8e54acdb64.jpg"
      },
      "metadata": {
        "table_type": "simple_table",
        "row_count": 5,
        "col_count": 2
      }
    },
    {
      "id": "REFRAG_elem_000148",
      "type": "table",
      "content": {
        "html": "<table><tr><td></td><td>P16</td><td>P32</td><td>P128</td><td>P2048 ↓</td></tr><tr><td>LLAMA-FULL CONTEXT</td><td>1.397</td><td>0.734</td><td>0.203</td><td>0.021</td></tr><tr><td>LLAMA-No CONTEXT</td><td>3.483</td><td>2.981</td><td>2.249</td><td>1.590</td></tr><tr><td>REFRAG w/o curriculum</td><td>3.719</td><td>3.098</td><td>2.272</td><td>1.599</td></tr><tr><td>REFRAG with curriculum</td><td>0.669</td><td>0.451</td><td>0.230</td><td>0.135</td></tr></table>",
        "captions": [
          "Table 11 Performance comparison on reconstruction task with and w/o curriculum learning. Log-Perplexity is reported as average of Arxiv and Book domain. "
        ],
        "description": ""
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 25,
        "bbox": {
          "x1": 277,
          "y1": 132,
          "x2": 718,
          "y2": 208
        },
        "section_title": "C.1 Additional Contextual Application - Summarization Task ",
        "image_path": "D:/Code_list/Code_Python/Agent_gragh_rag/files/file_store/md_store/minerU_work/REFRAG/images/f5bd7dfef62c5849ec808e2be758a7a63d8fc244a32d29f188e3e309f30153b8.jpg"
      },
      "metadata": {
        "table_type": "simple_table",
        "row_count": 4,
        "col_count": 5
      }
    },
    {
      "id": "REFRAG_elem_000149",
      "type": "table",
      "content": {
        "html": "<table><tr><td></td><td>P16</td><td>P32</td><td>P128</td><td>P2048 ↓</td></tr><tr><td>LLAMA-FULL CONTEXT</td><td>1.448</td><td>1.458</td><td>1.464</td><td>1.449</td></tr><tr><td>LLAMA-No CONTEXT</td><td>3.483</td><td>2.981</td><td>2.249</td><td>1.590</td></tr><tr><td>REFRAG w/o reconstruction</td><td>3.272</td><td>2.789</td><td>2.119</td><td>1.544</td></tr><tr><td>REFRAG with reconstruction</td><td>2.017</td><td>1.837</td><td>1.632</td><td>1.453</td></tr></table>",
        "captions": [
          "Table 12 Performance comparison on continual pre-training task with and w/o continued from reconstruction task. Log-Perplexity is reported as average of Arxiv and Book domain. "
        ],
        "description": ""
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 25,
        "bbox": {
          "x1": 267,
          "y1": 282,
          "x2": 730,
          "y2": 359
        },
        "section_title": "C.1 Additional Contextual Application - Summarization Task ",
        "image_path": "D:/Code_list/Code_Python/Agent_gragh_rag/files/file_store/md_store/minerU_work/REFRAG/images/c3c17259ba2d059e31721f8d4bfa37b2b05a589b840db0c5963d018e41f79914.jpg"
      },
      "metadata": {
        "table_type": "simple_table",
        "row_count": 4,
        "col_count": 5
      }
    },
    {
      "id": "REFRAG_elem_000150",
      "type": "table",
      "content": {
        "html": "<table><tr><td></td><td></td><td colspan=\"3\">Arxiv</td><td colspan=\"3\">Book</td><td colspan=\"3\">PG19</td><td colspan=\"3\">ProofPile</td></tr><tr><td></td><td>Compression Rate</td><td>P512</td><td>P1024</td><td>P2048</td><td>P512</td><td>P1024</td><td>P2048</td><td>P512</td><td>P1024</td><td>P2048</td><td>P512</td><td>P1024</td><td>P2048 ↓</td></tr><tr><td colspan=\"14\">Context Length=2048</td></tr><tr><td>REFRAG8</td><td>8</td><td>1.124</td><td>1.091</td><td>1.062</td><td>1.905</td><td>1.868</td><td>1.844</td><td>1.996</td><td>1.956</td><td>1.927</td><td>0.997</td><td>0.952</td><td>0.916</td></tr><tr><td>REFRAG16+RL</td><td>8.258</td><td>1.118</td><td>1.090</td><td>1.062</td><td>1.878</td><td>1.856</td><td>1.840</td><td>1.978</td><td>1.952</td><td>1.930</td><td>0.992</td><td>0.951</td><td>0.916</td></tr><tr><td colspan=\"14\">Context Length=4096</td></tr><tr><td>REFRAG8</td><td>8</td><td>1.098</td><td>1.065</td><td>1.042</td><td>1.895</td><td>1.860</td><td>1.837</td><td>1.989</td><td>1.950</td><td>1.922</td><td>0.965</td><td>0.923</td><td>0.894</td></tr><tr><td>REFRAG16+RL</td><td>8.0157</td><td>1.065</td><td>1.048</td><td>1.033</td><td>1.851</td><td>1.837</td><td>1.828</td><td>1.952</td><td>1.934</td><td>1.918</td><td>0.932</td><td>0.905</td><td>0.883</td></tr></table>",
        "captions": [
          "Table 13 The performance of REFRAG under the same compression rate with full compression (i.e., REFRAG8) and selective compression (i.e., REFRAG16+RL). "
        ],
        "description": ""
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 25,
        "bbox": {
          "x1": 116,
          "y1": 435,
          "x2": 885,
          "y2": 529
        },
        "section_title": "C.1 Additional Contextual Application - Summarization Task ",
        "image_path": "D:/Code_list/Code_Python/Agent_gragh_rag/files/file_store/md_store/minerU_work/REFRAG/images/96d2b55833137ed8295a36cce2e48338a9d9bea15b66be86f65422aa51501226.jpg"
      },
      "metadata": {
        "table_type": "complex_table",
        "row_count": 7,
        "col_count": 9
      }
    },
    {
      "id": "REFRAG_elem_000151",
      "type": "table",
      "content": {
        "html": "<table><tr><td rowspan=\"2\">Encoder-Decoder</td><td rowspan=\"2\">Context Length</td><td colspan=\"3\">LLaMA-3.1-8B</td><td colspan=\"3\">LLaMA-3.2-3B</td></tr><tr><td>P512</td><td>P1024</td><td>P2048</td><td>P512</td><td>P1024</td><td>P2048 ↓</td></tr><tr><td>Full Context</td><td>2048</td><td>1.000</td><td>0.989</td><td>0.972</td><td>1.092</td><td>1.080</td><td>1.062</td></tr><tr><td>No Context</td><td>0</td><td>1.445</td><td>1.286</td><td>1.162</td><td>1.559</td><td>1.392</td><td>1.262</td></tr><tr><td>Roberta-Base</td><td>2048</td><td>1.109</td><td>1.067</td><td>1.026</td><td>1.175</td><td>1.133</td><td>1.093</td></tr><tr><td>Roberta-Large</td><td>2048</td><td>1.107</td><td>1.065</td><td>1.025</td><td>1.170</td><td>1.130</td><td>1.091</td></tr><tr><td>Roberta-Base</td><td>4096</td><td>1.067</td><td>1.032</td><td>0.999</td><td>1.142</td><td>1.105</td><td>1.070</td></tr><tr><td>Roberta-Large</td><td>4096</td><td>1.065</td><td>1.031</td><td>0.998</td><td>1.130</td><td>1.096</td><td>1.064</td></tr></table>",
        "captions": [
          "Table 14 Log-Perplexity of continual pre-training for different encoder-decoder combinations. Lower log-perplexity indicates better performance. "
        ],
        "description": ""
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 25,
        "bbox": {
          "x1": 230,
          "y1": 604,
          "x2": 771,
          "y2": 705
        },
        "section_title": "C.1 Additional Contextual Application - Summarization Task ",
        "image_path": "D:/Code_list/Code_Python/Agent_gragh_rag/files/file_store/md_store/minerU_work/REFRAG/images/90029671ef183397d52856d615019bac4605ab5316022629f34e94503f31d615.jpg"
      },
      "metadata": {
        "table_type": "complex_table",
        "row_count": 7,
        "col_count": 7
      }
    },
    {
      "id": "REFRAG_elem_000152",
      "type": "table",
      "content": {
        "html": "<table><tr><td># Passages</td><td>MMLU</td><td>NQ</td><td>FEVER</td><td>WebQA</td><td>FreebaseQA</td><td>CommonsenseQA</td><td>ECQA</td><td>StrategyQA</td><td>HellaSwag</td><td>SIQA</td><td>PIQA ↑</td></tr><tr><td>0</td><td>48.07</td><td>18.73</td><td>65.80</td><td>34.67</td><td>60.20</td><td>89.18</td><td>87.42</td><td>68.89</td><td>43.72</td><td>67.25</td><td>70.18</td></tr><tr><td>1</td><td>50.49</td><td>21.39</td><td>69.46</td><td>37.33</td><td>68.06</td><td>86.60</td><td>89.40</td><td>80.00</td><td>43.26</td><td>68.17</td><td>70.08</td></tr><tr><td>3</td><td>50.49</td><td>22.01</td><td>66.02</td><td>38.67</td><td>71.01</td><td>89.18</td><td>95.36</td><td>71.11</td><td>45.50</td><td>68.73</td><td>71.44</td></tr><tr><td>5</td><td>50.62</td><td>23.00</td><td>66.07</td><td>41.33</td><td>72.48</td><td>91.75</td><td>96.03</td><td>75.56</td><td>45.48</td><td>68.17</td><td>71.38</td></tr><tr><td>8</td><td>50.29</td><td>22.96</td><td>66.59</td><td>38.67</td><td>73.46</td><td>92.27</td><td>94.70</td><td>75.56</td><td>45.23</td><td>68.94</td><td>71.38</td></tr><tr><td>20</td><td>51.01</td><td>24.30</td><td>67.77</td><td>40.00</td><td>75.18</td><td>91.75</td><td>98.01</td><td>75.56</td><td>45.09</td><td>68.53</td><td>71.00</td></tr><tr><td>50</td><td>51.08</td><td>24.76</td><td>69.39</td><td>40.00</td><td>75.92</td><td>91.75</td><td>97.35</td><td>75.56</td><td>44.78</td><td>67.81</td><td>69.97</td></tr><tr><td>80</td><td>50.42</td><td>24.15</td><td>68.83</td><td>37.33</td><td>74.20</td><td>92.27</td><td>97.35</td><td>71.11</td><td>44.61</td><td>68.22</td><td>69.37</td></tr><tr><td>100</td><td>50.23</td><td>23.99</td><td>69.80</td><td>36.00</td><td>74.45</td><td>92.27</td><td>97.35</td><td>71.11</td><td>44.57</td><td>68.07</td><td>69.75</td></tr></table>",
        "captions": [
          "Table 15 Performance of our model under compression rate of 16 with different number of retrieved passages in RAG under the strong retriever scenario. "
        ],
        "description": ""
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 25,
        "bbox": {
          "x1": 116,
          "y1": 782,
          "x2": 880,
          "y2": 897
        },
        "section_title": "C.1 Additional Contextual Application - Summarization Task ",
        "image_path": "D:/Code_list/Code_Python/Agent_gragh_rag/files/file_store/md_store/minerU_work/REFRAG/images/608aa1eb5c1b764558db311b8131819bfda9ee0811587f1de9e0c8da10c76010.jpg"
      },
      "metadata": {
        "table_type": "simple_table",
        "row_count": 9,
        "col_count": 12
      }
    },
    {
      "id": "REFRAG_elem_000153",
      "type": "table",
      "content": {
        "html": "<table><tr><td>Generation</td><td>NQ</td><td>FEVER</td><td>TQA</td><td>WebQA</td><td>FreebaseQA</td><td>GSM8K</td><td>StrategyQA</td><td>BoolQ ↑</td></tr><tr><td>\\( LLAMA_{FT} \\)</td><td>21.88</td><td>61.85</td><td>7.96</td><td>34.67</td><td>72.97</td><td>8.72</td><td>71.11</td><td>29.54</td></tr><tr><td>CEPE</td><td>0.05</td><td>60.68</td><td>0.01</td><td>0.00</td><td>0.25</td><td>0.00</td><td>0.00</td><td>56.70</td></tr><tr><td>REPLUG</td><td>14.96</td><td>71.56</td><td>11.01</td><td>25.33</td><td>53.32</td><td>4.70</td><td>66.67</td><td>3.15</td></tr><tr><td>LLAMA-32K</td><td>2.26</td><td>0.23</td><td>2.17</td><td>14.67</td><td>9.83</td><td>0.67</td><td>4.44</td><td>0.06</td></tr><tr><td>\\( REFRAG_8 \\)</td><td>20.86</td><td>63.44</td><td>12.37</td><td>38.67</td><td>65.60</td><td>11.41</td><td>73.33</td><td>3.06</td></tr><tr><td>\\( REFRAG_{16} \\)</td><td>20.60</td><td>60.45</td><td>11.86</td><td>40.00</td><td>66.09</td><td>11.41</td><td>73.33</td><td>5.57</td></tr><tr><td>\\( REFRAG_{32} \\)</td><td>21.39</td><td>61.97</td><td>12.03</td><td>40.00</td><td>67.32</td><td>12.75</td><td>68.89</td><td>1.80</td></tr><tr><td>Multi-Choice</td><td>MMLU</td><td>CommonsenseQA</td><td>MathQA</td><td>ECQA</td><td>HellaSwag</td><td>SIQA</td><td>PIQA</td><td>Winogrande ↑</td></tr><tr><td>\\( LLAMA_{FT} \\)</td><td>49.97</td><td>84.02</td><td>97.48</td><td>86.09</td><td>42.78</td><td>67.09</td><td>68.39</td><td>54.78</td></tr><tr><td>CEPE</td><td>26.06</td><td>20.62</td><td>24.16</td><td>19.87</td><td>24.99</td><td>33.57</td><td>49.13</td><td>46.96</td></tr><tr><td>REPLUG</td><td>47.35</td><td>77.84</td><td>99.50</td><td>79.47</td><td>49.26</td><td>64.99</td><td>71.98</td><td>56.04</td></tr><tr><td>LLAMA-32K</td><td>24.17</td><td>18.04</td><td>22.32</td><td>15.89</td><td>24.09</td><td>16.84</td><td>28.02</td><td>48.78</td></tr><tr><td>\\( REFRAG_8 \\)</td><td>49.90</td><td>91.24</td><td>99.66</td><td>97.35</td><td>45.03</td><td>68.27</td><td>70.95</td><td>57.22</td></tr><tr><td>\\( REFRAG_{16} \\)</td><td>49.84</td><td>90.21</td><td>99.66</td><td>96.69</td><td>39.52</td><td>68.63</td><td>70.95</td><td>56.35</td></tr><tr><td>\\( REFRAG_{32} \\)</td><td>49.84</td><td>91.24</td><td>99.50</td><td>97.35</td><td>42.71</td><td>68.32</td><td>68.72</td><td>56.12</td></tr></table>",
        "captions": [
          "Table 16 Comparison of model performance of different models with different number of retrieved chunks for RAG. The number of contexts in all the evaluation here is 5. "
        ],
        "description": ""
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 26,
        "bbox": {
          "x1": 117,
          "y1": 130,
          "x2": 879,
          "y2": 339
        },
        "section_title": "C.1 Additional Contextual Application - Summarization Task ",
        "image_path": "D:/Code_list/Code_Python/Agent_gragh_rag/files/file_store/md_store/minerU_work/REFRAG/images/1d477dbc4cf3885d92e5697b52014ab4e9826b52756ef2bd11672f671becfab8.jpg"
      },
      "metadata": {
        "table_type": "simple_table",
        "row_count": 15,
        "col_count": 9
      }
    },
    {
      "id": "REFRAG_elem_000154",
      "type": "table",
      "content": {
        "html": "<table><tr><td>Generation</td><td>NQ</td><td>FEVER</td><td>TQA</td><td>WebQA</td><td>FreebaseQA</td><td>GSM8K</td><td>StrategyQA</td><td>BoolQ ↑</td><td>(1/ # tokens)</td></tr><tr><td colspan=\"10\">Short context with the same latency</td></tr><tr><td>\\( LLAMA_{FT} + 1 \\) passage</td><td>20.20</td><td>57.70</td><td>8.32</td><td>32.00</td><td>67.08</td><td>6.71</td><td>62.22</td><td>31.25</td><td>1×</td></tr><tr><td>\\( REFRAG_{8} + 8 \\) passages</td><td>21.22</td><td>63.21</td><td>11.77</td><td>42.67</td><td>67.57</td><td>8.72</td><td>68.89</td><td>3.24</td><td>1×</td></tr><tr><td>\\( REFRAG_{16} + 8 \\) passages</td><td>20.73</td><td>60.86</td><td>11.60</td><td>40.00</td><td>66.83</td><td>11.41</td><td>77.78</td><td>6.36</td><td>2×</td></tr><tr><td>\\( REFRAG_{32} + 8 \\) passages</td><td>21.08</td><td>62.65</td><td>11.69</td><td>42.67</td><td>66.58</td><td>11.41</td><td>68.89</td><td>2.35</td><td>4×</td></tr><tr><td colspan=\"10\">Long context</td></tr><tr><td>\\( LLAMA_{FT} + 10 \\) passages</td><td>22.27</td><td>60.40</td><td>8.32</td><td>38.67</td><td>71.50</td><td>9.40</td><td>71.11</td><td>29.94</td><td>1×</td></tr><tr><td>CEPED +80 passages</td><td>0.02</td><td>65.18</td><td>0.02</td><td>0.00</td><td>0.00</td><td>0.00</td><td>0.00</td><td>59.33</td><td></td></tr><tr><td>REPLUG +80 passages</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>64.44</td><td>-</td><td></td></tr><tr><td>\\( LLAMA-32K +80 \\) passages</td><td>1.03</td><td>0.12</td><td>0.37</td><td>5.33</td><td>9.34</td><td>0.00</td><td>0.00</td><td>0.03</td><td></td></tr><tr><td>\\( REFRAG_{8} +80 \\) passages</td><td>22.92</td><td>67.87</td><td>12.22</td><td>46.67</td><td>71.99</td><td>10.07</td><td>68.89</td><td>7.19</td><td>1×</td></tr><tr><td>\\( REFRAG_{16} +80 \\) passages</td><td>22.63</td><td>65.07</td><td>12.12</td><td>38.67</td><td>71.74</td><td>8.72</td><td>68.89</td><td>12.05</td><td>2×</td></tr><tr><td>\\( REFRAG_{32} +80 \\) passages</td><td>21.86</td><td>67.24</td><td>11.54</td><td>41.33</td><td>70.76</td><td>8.72</td><td>66.67</td><td>6.30</td><td>4×</td></tr><tr><td>Multi-Choice</td><td>MMLU</td><td>CommonsenseQA</td><td>MathQA</td><td>ECQA</td><td>HellaSwag</td><td>SIQA</td><td>PIQA</td><td>Winogrande ↑</td><td></td></tr><tr><td colspan=\"10\">Short context with the same latency</td></tr><tr><td>\\( LLAMA_{FT} + 1 \\) context</td><td>48.86</td><td>82.99</td><td>99.50</td><td>84.77</td><td>42.08</td><td>67.91</td><td>67.46</td><td>55.49</td><td>1×</td></tr><tr><td>\\( REFRAG_{8} + 8 \\) passages</td><td>50.10</td><td>91.24</td><td>99.66</td><td>96.03</td><td>45.15</td><td>68.17</td><td>70.40</td><td>57.46</td><td>1×</td></tr><tr><td>\\( REFRAG_{16} + 8 \\) passages</td><td>49.77</td><td>90.21</td><td>99.66</td><td>96.69</td><td>39.32</td><td>68.73</td><td>70.46</td><td>56.43</td><td>2×</td></tr><tr><td>\\( REFRAG_{32} + 8 \\) passages</td><td>50.10</td><td>91.75</td><td>99.50</td><td>96.03</td><td>42.36</td><td>68.83</td><td>68.28</td><td>55.80</td><td>4×</td></tr><tr><td colspan=\"10\">Long context</td></tr><tr><td>\\( LLAMA_{FT} + 10 \\) passages</td><td>45.20</td><td>83.51</td><td>63.42</td><td>85.43</td><td>41.43</td><td>67.60</td><td>67.36</td><td>54.30</td><td>1×</td></tr><tr><td>CEPED +80 passages</td><td>26.52</td><td>24.74</td><td>23.83</td><td>22.52</td><td>24.97</td><td>32.86</td><td>48.80</td><td>44.20</td><td></td></tr><tr><td>REPLUG +80 passages</td><td>-</td><td>-</td><td>-</td><td>76.16</td><td>-</td><td>65.46</td><td>-</td><td>55.33</td><td></td></tr><tr><td>\\( LLAMA-32K +80 \\) passages</td><td>22.01</td><td>18.04</td><td>19.97</td><td>16.56</td><td>23.69</td><td>23.80</td><td>33.19</td><td>48.62</td><td></td></tr><tr><td>\\( REFRAG_{8} +80 \\) passages</td><td>50.03</td><td>90.72</td><td>99.66</td><td>97.35</td><td>44.44</td><td>67.66</td><td>69.48</td><td>56.91</td><td>1×</td></tr><tr><td>\\( REFRAG_{16} +80 \\) passages</td><td>49.77</td><td>90.21</td><td>99.66</td><td>95.36</td><td>38.29</td><td>68.12</td><td>70.57</td><td>56.91</td><td>2×</td></tr><tr><td>\\( REFRAG_{32} +80 \\) passages</td><td>50.03</td><td>91.24</td><td>99.50</td><td>98.01</td><td>43.02</td><td>68.58</td><td>68.55</td><td>57.22</td><td>4×</td></tr></table>",
        "captions": [
          "Table 17 Comparison of model performance of different models with different number of retrieved passages for RAG under the weak retriever scenario. "
        ],
        "description": ""
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 26,
        "bbox": {
          "x1": 116,
          "y1": 412,
          "x2": 879,
          "y2": 699
        },
        "section_title": "C.1 Additional Contextual Application - Summarization Task ",
        "image_path": "D:/Code_list/Code_Python/Agent_gragh_rag/files/file_store/md_store/minerU_work/REFRAG/images/39f2c365eee70ccc3899f98e8b264702e5456b97a58e0448c64c77c939012537.jpg"
      },
      "metadata": {
        "table_type": "complex_table",
        "row_count": 27,
        "col_count": 8
      }
    },
    {
      "id": "REFRAG_elem_000155",
      "type": "table",
      "content": {
        "html": "<table><tr><td># Passages</td><td>MMLU</td><td>NQ</td><td>FEVER</td><td>WebQA</td><td>FreebaseQA</td><td>CommonsenseQA</td><td>ECQA</td><td>StrategyQA</td><td>HellaSwag</td><td>SIQA</td><td>PIQA↑</td></tr><tr><td>0</td><td>48.14</td><td>19.09</td><td>61.40</td><td>30.67</td><td>59.71</td><td>85.05</td><td>86.75</td><td>55.56</td><td>36.57</td><td>64.59</td><td>68.82</td></tr><tr><td>1</td><td>49.97</td><td>20.08</td><td>64.15</td><td>38.67</td><td>64.62</td><td>87.63</td><td>92.72</td><td>71.11</td><td>39.08</td><td>68.58</td><td>70.57</td></tr><tr><td>3</td><td>49.64</td><td>20.63</td><td>60.80</td><td>40.00</td><td>68.55</td><td>89.69</td><td>95.36</td><td>75.56</td><td>39.41</td><td>69.40</td><td>71.11</td></tr><tr><td>5</td><td>49.84</td><td>20.60</td><td>60.45</td><td>40.00</td><td>66.09</td><td>90.21</td><td>96.69</td><td>73.33</td><td>39.52</td><td>68.63</td><td>70.95</td></tr><tr><td>8</td><td>49.77</td><td>20.73</td><td>60.86</td><td>40.00</td><td>66.83</td><td>90.21</td><td>96.69</td><td>77.78</td><td>39.32</td><td>68.73</td><td>70.46</td></tr><tr><td>20</td><td>50.03</td><td>21.29</td><td>62.32</td><td>36.00</td><td>68.06</td><td>89.69</td><td>95.36</td><td>75.56</td><td>38.58</td><td>69.29</td><td>70.62</td></tr><tr><td>50</td><td>49.84</td><td>22.12</td><td>63.54</td><td>37.33</td><td>71.99</td><td>89.69</td><td>96.69</td><td>75.56</td><td>38.11</td><td>68.53</td><td>70.84</td></tr><tr><td>80</td><td>49.77</td><td>22.63</td><td>65.07</td><td>38.67</td><td>71.74</td><td>90.21</td><td>95.36</td><td>68.89</td><td>38.29</td><td>68.12</td><td>70.57</td></tr><tr><td>100</td><td>50.62</td><td>22.80</td><td>65.17</td><td>37.33</td><td>73.46</td><td>89.69</td><td>96.03</td><td>68.89</td><td>38.51</td><td>68.37</td><td>70.18</td></tr></table>",
        "captions": [
          "Table 18 Performance of our model under compression rate of 16 with different number of retrieved passages in RAG under the weak retriever scenario. "
        ],
        "description": ""
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 26,
        "bbox": {
          "x1": 116,
          "y1": 782,
          "x2": 879,
          "y2": 897
        },
        "section_title": "C.1 Additional Contextual Application - Summarization Task ",
        "image_path": "D:/Code_list/Code_Python/Agent_gragh_rag/files/file_store/md_store/minerU_work/REFRAG/images/b3dbf0483ac41f0ff7d0250b67a4fb4d889a8ea908ff086f4cbc10f2b51290d8.jpg"
      },
      "metadata": {
        "table_type": "simple_table",
        "row_count": 9,
        "col_count": 12
      }
    },
    {
      "id": "REFRAG_elem_000156",
      "type": "title",
      "content": {
        "text": "Ground True Abstract ",
        "level": 1
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 27,
        "bbox": {
          "x1": 122,
          "y1": 253,
          "x2": 245,
          "y2": 263
        }
      },
      "metadata": {
        "char_count": 21
      }
    },
    {
      "id": "REFRAG_elem_000157",
      "type": "paragraph",
      "content": {
        "text": "background : timely access to cardiovascular health services is necessary to prevent heart damages . the present study examined inequality in geographical distribution of cardiovascular health services in iran . methods : present study is a cross - sectional study conducted using demographic data from all iranian provinces ( 31 provinces ) from 2012 census by the statistics center of iran ( sci ) . the gini coefficients of ccu beds and cardiologists were used to assess equality in access to cardiovascular health services in iran . ms excel software was used to calculate gini coefficients . results : the proportions of ccu bed and cardiologist per 100,000 population were 4.88 and 1.27 , respectively ; also the gini coefficients were 0.129 and 0.045 , respectively . conclusion : descriptive statistics showed a skewness in distribution of pubic cardiovascular health services in iran , though gini coefficient revealed no significant inequality . however , equal distribution of ccu beds and cardiovascular specialists does not mean they are sufficiently available in iran . "
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 27,
        "bbox": {
          "x1": 122,
          "y1": 266,
          "x2": 493,
          "y2": 455
        },
        "section_title": "Ground True Abstract "
      },
      "metadata": {
        "char_count": 1084
      }
    },
    {
      "id": "REFRAG_elem_000158",
      "type": "paragraph",
      "content": {
        "text": "lumbar spinal stenosis is a commonly treated with epidural injections of local anesthetics and corticosteroids , however , these therapies may relieve leg pain for weeks to months but do not influence functional status . furthermore , the majority of patients report no substantial symptom change over the repeated treatment . utilizing balloon catheters , we successfully treated with three patients who complained persistent symptoms despite repeated conventional steroid injections . our results suggest that transforaminal decompression using a balloon catheter may have potential in the nonsurgical treatment of spinal stenosis by modifying the underlying pathophysiology . "
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 27,
        "bbox": {
          "x1": 122,
          "y1": 469,
          "x2": 493,
          "y2": 599
        },
        "section_title": "Ground True Abstract "
      },
      "metadata": {
        "char_count": 679
      }
    },
    {
      "id": "REFRAG_elem_000159",
      "type": "paragraph",
      "content": {
        "text": "we describe a 26-year - old woman who presented with a nodular rash on the elbows following an insect bite . two days later , she developed erythema nodosum . both these lesions were treated symptomatically . one week later , she had purpura , abdominal pain , hematuria , and arthralgias , following which steroids were administered . her investigations revealed only microscopic hematuria that disappeared with therapy . this pattern of sequential appearance of rash and a nodular morphology are both unique features not previously reported . "
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 27,
        "bbox": {
          "x1": 122,
          "y1": 601,
          "x2": 493,
          "y2": 708
        },
        "section_title": "Ground True Abstract "
      },
      "metadata": {
        "char_count": 545
      }
    },
    {
      "id": "REFRAG_elem_000160",
      "type": "title",
      "content": {
        "text": "Generated Abstract ",
        "level": 1
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 27,
        "bbox": {
          "x1": 506,
          "y1": 253,
          "x2": 617,
          "y2": 263
        }
      },
      "metadata": {
        "char_count": 19
      }
    },
    {
      "id": "REFRAG_elem_000161",
      "type": "paragraph",
      "content": {
        "text": "background : this study aimed to investigate the inequality of distribution of cardiac care units ( ccu ) and cardiologists in iran . methods : this study used demographic data from national statistics collected by the central statistics of iran ( sci ) in 2012 . the number of ccu beds and cardiologists per 100,000 individuals and the number of cardiologists per 10 ccu beds were explored . the gini coefficient was applied to measure inequality . results : the mean number of ccu beds per 100,000 individuals in iran was 4.88 , which is out of a total of 31 provinces below the national mean . the mean number of cardiologists per 100,000 individuals was 1.27 and the mean number of cardiologists per 10 ccu beds was 2.6 . the gini coefficient of cardiologists in 2012 was 0.045 and the gini coefficient of ccu beds was 0.129 . conclusion : our descriptive statistics showed that there is a skewness in the distribution of pubic cardiovascular health services in iran . moreover , the equal distribution of cardiovascular health facilities such as ccu beds is not necessarily provided in iran . "
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 27,
        "bbox": {
          "x1": 506,
          "y1": 266,
          "x2": 877,
          "y2": 468
        },
        "section_title": "Generated Abstract "
      },
      "metadata": {
        "char_count": 1098
      }
    },
    {
      "id": "REFRAG_elem_000162",
      "type": "paragraph",
      "content": {
        "text": "epidural injection is a common treatment for spinal stenosis . however , there is little information on the optimal management of spinal stenosis . we describe the use of epidural balloon catheters to decompress the intervertebral foramen in three patients with spinal stenosis . patients were followed - up for 24 weeks . one patient reported moderate pain relief , three patients reported symptom improvement and one patient reported no change in symptoms . this report suggests that transforaminal balloon decompression using a balloon may have potential in the nonsurgical treatment of spinal stenosis by modifying the underlying pathophysiology of segmental spinal stenosis . "
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 27,
        "bbox": {
          "x1": 506,
          "y1": 469,
          "x2": 877,
          "y2": 599
        },
        "section_title": "Generated Abstract "
      },
      "metadata": {
        "char_count": 681
      }
    },
    {
      "id": "REFRAG_elem_000163",
      "type": "paragraph",
      "content": {
        "text": "we report a case of herpes simplex purpura ( hsp ) that presented with a sequential pattern of rashes following an insect bite . the patient was a 26-year - old woman who presented to our outpatient department ( opd ) with a nodular rash on her elbows and erythema nodosum on her lower limbs following an insect bite . she had purpura on her lower limbs 2 weeks later . she had similar lesions on both upper and lower limbs 1 week after a second insect bite . this pattern of rashes has not been previously reported in hsp . "
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 27,
        "bbox": {
          "x1": 506,
          "y1": 599,
          "x2": 877,
          "y2": 696
        },
        "section_title": "Generated Abstract "
      },
      "metadata": {
        "char_count": 525
      }
    },
    {
      "id": "REFRAG_elem_000164",
      "type": "paragraph",
      "content": {
        "text": "Table 19 Comparison of ground true abstract and abstract generated by REFRAG for PubMed. "
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 27,
        "bbox": {
          "x1": 111,
          "y1": 720,
          "x2": 751,
          "y2": 734
        },
        "section_title": "Generated Abstract "
      },
      "metadata": {
        "char_count": 89
      }
    },
    {
      "id": "REFRAG_elem_000165",
      "type": "table",
      "content": {
        "html": "<table><tr><td>Ground True Abstract</td><td>Generated Abstract</td></tr><tr><td>memory - based learning , keeping full memory of learning material, appears a viable approach to learning nlp tasks , and is often superior in generalisation accuracy to eager learning approaches that abstract from learning material . here we investigate three partial memory - based learning approaches which remove from memory specific task instance types estimated to be exceptional . the three approaches each implement one heuristic function for estimating exceptionality of instance types : ( i ) typicality , ( ii ) class prediction strength , and ( iii ) friendly - neighbourhood size . experiments are performed with the memory - based learning algorithm ib1-ig trained on english word pronunciation . we find that removing instance types with low prediction strength ( ii ) is the only tested method which does not seriously harm generalisation accuracy . we conclude that keeping full memory of types rather than tokens , and excluding minority ambiguities appear to be the only performance - preserving optimisations of memory - based learning .</td><td>memory - based learning is a prominent and successful approach to machine learning . however , its use in nlp tasks has been limited by its over - generalisation properties . this paper investigates two optimisations that can be applied to memory - based learning in order to improve its generalisation performance : ( 1 ) replacing instance types ( memory tokens ) by instance types with frequency information , and ( 2 ) removing redundant information ( i.e . , instance types with low predictive power ) . we perform experiments on a large data set of english word pronunciations . we show that both optimisations yield improvements in generalisation performance . our results also indicate that atypicality , non - typicality , and friendly - neighbourhood size are estimates of the importance of instance types for their classification , rather than their removability .</td></tr><tr><td>machine learning methods are applied to finding the green s function of the anderson impurity model , a basic model system of quantum many - body condensed - matter physics . different methods of parametrizing the green s function are investigated ; a representation in terms of legendre polynomials is found to be superior due to its limited number of coefficients and its applicability to state of the art methods of solution . the dependence of the errors on the size of the training set is determined . the results indicate that a machine learning approach to dynamical mean - field theory may be feasible .</td><td>ewe demonstrate how machine learning techniques can be used to solve the quantum many - body problem . we apply the method to the anderson impurity model , where we use the exact diagonalization method to train the machine learning algorithms . we show that machine learning is an efficient and accurate method to compute the self - energy of the model and to predict the spectral function of the model . we also show that machine learning algorithms can be used to efficiently compute the self - consistent green s function starting from any hybridization function .</td></tr><tr><td>particle swarm optimization is used in several combinatorial optimization problems . in this work , particle swarms are used to solve quadratic programming problems with quadratic constraints . the approach of particle swarms is an example for interior point methods in optimization as an iterative technique . this approach is novel and deals with classification problems without the use of a traditional classifier . our method determines the optimal hyperplane or classification boundary for a data set . in a binary classification problem , we constrain each class as a cluster , which is enclosed by an ellipsoid . the estimation of the optimal hyperplane between the two clusters is posed as a quadratically constrained quadratic problem . the optimization problem is solved in distributed format using modified particle swarms . our method has the advantage of using the direction towards optimal solution rather than searching the entire feasible region . our results on the iris , pima , wine , and thyroid datasets show that the proposed method works better than a neural network and the performance is close to that ofsvm . * keywords * quadratic programming; particle swarms ; hyperplane ; quadratic constraints ; binary classification .</td><td>support vector machines are used for classification of data in machine learning . support vector machines use quadratic programming formulation for minimizing the objective function . the quadratic programming problem is solved by particle swarm optimization . the proposed method is compared with khachiya s and karman s support vector machine algorithms for linear and neural networks and quadratic programming . the results show that the proposed method is better than the other two methods .</td></tr></table>",
        "captions": [
          "Table 20 Comparison of ground true abstract and abstract generated by REFRAG for ArXiv. "
        ],
        "description": ""
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 28,
        "bbox": {
          "x1": 116,
          "y1": 191,
          "x2": 885,
          "y2": 770
        },
        "section_title": "Generated Abstract ",
        "image_path": "D:/Code_list/Code_Python/Agent_gragh_rag/files/file_store/md_store/minerU_work/REFRAG/images/69e81b7d9e0fc9110065162a3fa79f38e83c78e800cc79d09d189478b9117579.jpg"
      },
      "metadata": {
        "table_type": "simple_table",
        "row_count": 3,
        "col_count": 2
      }
    },
    {
      "id": "REFRAG_elem_000166",
      "type": "table",
      "content": {
        "html": "<table><tr><td></td><td colspan=\"3\">Arxiv</td><td colspan=\"3\">Pubmed</td></tr><tr><td></td><td>Rouge-1</td><td>Rouge-2</td><td>Rouge-L</td><td>Rouge-1</td><td>Rouge-2</td><td>Rouge-L ↑</td></tr><tr><td colspan=\"7\"># Decoder tokens = 128</td></tr><tr><td>\\(LLAMA_{FT}\\)</td><td>29.69</td><td>6.89</td><td>18.28</td><td>29.79</td><td>8.37</td><td>18.41</td></tr><tr><td>CEPED</td><td>12.67</td><td>1.66</td><td>8.39</td><td>12.01</td><td>1.41</td><td>7.74</td></tr><tr><td>\\(REPLUG_{FT}\\)</td><td>5.30</td><td>0.78</td><td>3.77</td><td>5.11</td><td>0.81</td><td>3.55</td></tr><tr><td>\\(REPLUG_{Chat}\\)</td><td>15.11</td><td>1.58</td><td>9.80</td><td>14.94</td><td>1.51</td><td>9.40</td></tr><tr><td>LLAMA-32K</td><td>2.83</td><td>0.48</td><td>2.11</td><td>7.94</td><td>1.63</td><td>5.31</td></tr><tr><td>\\(REFRAG_8\\)</td><td>36.50</td><td>12.48</td><td>22.21</td><td>38.27</td><td>13.91</td><td>23.20</td></tr><tr><td>\\(REFRAG_{16}\\)</td><td>38.48</td><td>12.50</td><td>22.66</td><td>38.93</td><td>12.83</td><td>23.07</td></tr><tr><td colspan=\"7\"># Decoder tokens =512</td></tr><tr><td>\\(LLAMA_{FT}\\)</td><td>36.03</td><td>11.16</td><td>21.49</td><td>38.15</td><td>14.36</td><td>23.27</td></tr><tr><td>CEPED</td><td>19.28</td><td>3.16</td><td>12.22</td><td>17.60</td><td>2.43</td><td>10.89</td></tr><tr><td>\\(REPLUG_{FT}\\)</td><td>28.33</td><td>6.42</td><td>17.04</td><td>28.29</td><td>7.59</td><td>16.97</td></tr><tr><td>\\(REPLUG_{Chat}\\)</td><td>31.41</td><td>7.00</td><td>18.32</td><td>30.67</td><td>7.13</td><td>17.56</td></tr><tr><td>LLAMA-32K</td><td>3.03</td><td>0.65</td><td>2.28</td><td>8.49</td><td>2.54</td><td>5.47</td></tr><tr><td>\\(REFRAG_8\\)</td><td>41.95</td><td>15.56</td><td>24.84</td><td>43.55</td><td>17.53</td><td>26.38</td></tr><tr><td colspan=\"7\"># Decoder tokens =1024</td></tr><tr><td>\\(LLAMA_{FT}\\)</td><td>41.24</td><td>15.07</td><td>24.45</td><td>42.45</td><td>17.58</td><td>26.11</td></tr><tr><td>CEPED</td><td>25.20</td><td>5.07</td><td>15.45</td><td>23.00</td><td>3.94</td><td>13.71</td></tr><tr><td>\\(REPLUG_{FT}\\)</td><td>19.32</td><td>3.18</td><td>12.73</td><td>17.07</td><td>2.93</td><td>11.20</td></tr><tr><td>\\(REPLUG_{Chat}\\)</td><td>27.38</td><td>5.46</td><td>16.84</td><td>27.89</td><td>5.16</td><td>15.93</td></tr><tr><td>LLAMA-32K</td><td>4.34</td><td>0.95</td><td>3.35</td><td>10.19</td><td>3.11</td><td>6.47</td></tr><tr><td>\\(REFRAG_8\\)</td><td>43.88</td><td>17.03</td><td>26.01</td><td>44.43</td><td>18.06</td><td>26.85</td></tr></table>",
        "captions": [
          "Table 21 Performance on summarization tasks under the same latency. "
        ],
        "description": ""
      },
      "source": {
        "file": "REFRAG.pdf",
        "page": 29,
        "bbox": {
          "x1": 174,
          "y1": 337,
          "x2": 823,
          "y2": 680
        },
        "section_title": "Generated Abstract ",
        "image_path": "D:/Code_list/Code_Python/Agent_gragh_rag/files/file_store/md_store/minerU_work/REFRAG/images/2c5b7f131d66e9509a6cf331a6e6d6f47f310367808eb6304b5b22487792aea9.jpg"
      },
      "metadata": {
        "table_type": "complex_table",
        "row_count": 23,
        "col_count": 6
      }
    }
  ]
}